{
  
    
        "post0": {
            "title": "ML Instant House Valuation: Part 4 - Model selection and implementation",
            "content": "import os, json import warnings from joblib import load, dump from tqdm import tqdm import numpy as np import pandas as pd from typing import Callable, Union from numpy import ndarray # ml algos dependencies from cuml import ( NearestNeighbors, # knn implementation dependency RandomForestRegressor, ForestInference # gpu accelerated front-end for xgboost, random forest etc. ) import xgboost as xgb import treelite # model compiler # hyperparameter optimisation from hyperopt import hp, fmin, tpe, Trials # visualisation from pprint import pformat import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (7, 5) plt.rcParams[&quot;figure.dpi&quot;] = 64 sns.set_theme(style=&quot;whitegrid&quot;) . . Final Preprocessing . df = pd.read_parquet(DATASET_PATH) df.head() . PRICE PRICE_ADJ YEAR LAT LNG TOTAL_FLOOR_AREA NUMBER_HABITABLE_ROOMS PROPERTY_TYPE . DATE_OF_TRANSFER . 1995-01-01 16000.0 | 605.142016 | 1995 | 53.723903 | -1.882344 | 99.00 | 4.0 | 1 | . 1995-01-02 35000.0 | 1323.748160 | 1995 | 52.245071 | -0.878169 | 85.00 | 4.0 | 1 | . 1995-01-03 15000.0 | 567.320640 | 1995 | 54.961605 | -1.419408 | 74.00 | 4.0 | 4 | . 1995-01-03 48000.0 | 1815.426048 | 1995 | 52.034265 | -2.432491 | 78.00 | 3.0 | 6 | . 1995-01-03 82000.0 | 3101.352831 | 1995 | 50.092749 | -5.265793 | 118.88 | 4.0 | 6 | . Scaling data . In order to use distance-based algorithms, which use distance between data points to determine similarity, I will have to scale the data to a closer range. Tree-based algorithms are fairly insensitive to the scaling of data since it only splits a node based on a single feature. Additionally, scaling the data will make results and models more interpretable, as you will more clearly be able to explain the strength of a particular feature determined by the value of its coefficient for the target variable. . I will use standardisation (Z-score normalisation) in order to rescale the dataset, calculated by the formula: $$x&#39; = frac{x- mu}{ sigma}$$ Where, $ mu$: mean of the dataset $ sigma$: standard deviation of the dataset . The standardised data will have zero mean and unit variance, therefore it will allow comparison between different features, which would usually have different unit scales. It is important I compute the mean and standard deviation using only the train set, as using the entire dataset could cause unintentional data leakage to the prediction model, i.e it could unintentionally provide information about unobserved data. . Another method I may try is normalisation or min-max scaling which rescales data to a given range. It is calculated with the formula: $$x&#39; = a + frac{(x-min(x))(b-a)}{max(x)-min(x)}$$ Where, $[a,b]$: interval in which the dataset will be scaled . Again, the min and max values should be computed using only the train set to prevent any unintentional data leakage to the prediction model. . I will create a class which will allow using both scaling methods, and I will attempt to implement an indexing method to return the scaler at a particular column. This will make it easier to inverse scaling on a prediction later on by instantiating a new scaler at the target&#39;s column index. . class Scaler: def __init__(self): pass # ============ Fitting ============ def fit(self, x): self._fit = np.array(x) # Fitting an array type for scaling return self def fit_transform(self, x, method=&quot;standardise&quot;, **kwargs): return self.fit(x), self.scale(x, method, **kwargs) # ============ Standardisation ============ def standardise(self, x): self._mean = self._fit.mean(axis=0) self._std = self._fit.std(axis=0) return (np.array(x) - self._mean)/self._std def inv_standardise(self,x): return np.array(x) * self._std + self._mean # ============ Normalisation/Min-Max scaling ============ def min_max(self, x, feature_range=(0,1)): assert len(feature_range) == 2, f&quot;Provide only 2 values for feature_range instead of {len(feature_range)} values&quot; self._a, self._b = feature_range self._min = self._fit.min(axis=0) self._max = self._fit.max(axis=0) return ((np.array(x)-self._min)*(self._b-self._a))/(self._max-self._min) + self._a def inv_min_max(self,x): return ((np.array(x)-self._a)*(self._max-self._min))/(self._b-self._a) + self._min # ============ General scaling ============ def scale(self, x, method:str=&quot;standardise&quot;, **kwargs): scale_method = { &quot;standardise&quot;:self.standardise, &quot;zscore&quot;:self.standardise, &quot;min_max&quot;:self.min_max, &quot;normalise&quot;:self.min_max, }[method] return scale_method(x) def inv(self, x, method=&quot;standardise&quot;,**kwargs): inv_method = { &quot;standardise&quot;:self.inv_standardise, &quot;zscore&quot;:self.inv_standardise, &quot;min_max&quot;:self.inv_min_max, &quot;normalise&quot;:self.inv_min_max, }[method] return inv_method(x) # ============ Serializing scaler ============ def serialize(self, names): serialized = {} if self._fit.ndim == 2: for i,name in enumerate(names): if hasattr(self, &quot;_mean&quot;): serialized[name] = { &quot;mean&quot;: self._mean[i], &quot;std&quot;: self._std[i] } if hasattr(self, &quot;_min&quot;): serialized[name] = { &quot;min&quot;: self._min[i], &quot;max&quot;: self._max[i] } else: if hasattr(self, &quot;_mean&quot;): serialized[names] = { &quot;mean&quot;: self._mean, &quot;std&quot;: self._std } if hasattr(self, &quot;_min&quot;): serialized[names] = { &quot;min&quot;: self._min, &quot;max&quot;: self._max } if hasattr(self,&quot;_a&quot;): serialized[&quot;minmax_range&quot;] = [self._a, self._b] return serialized . sample = df.sample(n=100_000) sample_std = Scaler().fit_transform(sample, method=&quot;standardise&quot;)[1] sample_norm = Scaler().fit_transform(sample, method=&quot;normalise&quot;)[1] f, axs = plt.subplots(1,3,sharey=True) sns.violinplot(data=sample.values, scale=&quot;width&quot;, orient=&quot;h&quot;, ax=axs[0]).set_xlabel(&quot;Original&quot;) sns.violinplot(data=sample_std, scale=&quot;width&quot;, orient=&quot;h&quot;, ax=axs[1]).set_xlabel(&quot;Standardised&quot;) sns.violinplot(data=sample_norm, scale=&quot;width&quot;, orient=&quot;h&quot;, ax=axs[2]).set_xlabel(&quot;Normalised&quot;) axs[0].set_xlim(-10, 125) axs[0].set_yticklabels([col[:6]+&quot;...&quot; if len(col) &gt; 9 else col for col in sample.columns]) axs[0].tick_params(&quot;y&quot;, labelsize=&quot;small&quot;) . Preprocessed data . from requests import get from bs4 import BeautifulSoup def getLatestIndex(): response = get(&quot;https://landregistry.data.gov.uk/app/ukhpi&quot;) soup = BeautifulSoup(response.content) return float(soup.find(class_=&quot;c-headline-figure__house-price-index&quot;).text.strip()[:-1]) LATEST_INDEX = getLatestIndex() LATEST_INDEX . 145.15 . def split(df, tr_pct=0.9, cb_pct=.05): tr_spl = int(len(df)*tr_pct) cb_spl = tr_spl + int(len(df)*cb_pct) return df[:tr_spl], df[tr_spl:cb_spl], df[cb_spl:] . X, y, y2 = df.drop(columns=[&quot;PRICE&quot;, &quot;PRICE_ADJ&quot;]), df.PRICE, df.PRICE_ADJ np.random.seed(SEED) tr_idx, cal_idx, val_idx = split(np.random.permutation(len(df))) # indices for shuffled dataset # fitting scalers on training data X_scaler, tr_X = Scaler().fit_transform(X.iloc[tr_idx]) y_scaler, tr_y = Scaler().fit_transform(y.iloc[tr_idx]) y2_scaler, tr_y2 = Scaler().fit_transform(y2.iloc[tr_idx]) # scaling and splitting validation/calibration data cal_X, val_X = X_scaler.scale(X.iloc[cal_idx]), X_scaler.scale(X.iloc[val_idx]) cal_y, val_y = y_scaler.scale(y.iloc[cal_idx]), y_scaler.scale(y.iloc[val_idx]) cal_y2, val_y2 = y2_scaler.scale(y2.iloc[cal_idx]), y2_scaler.scale(y2.iloc[val_idx]) inv_y = y.values[val_idx] inv_y2 = y2.values[val_idx] * LATEST_INDEX . SCALING_PATH = os.path.join(SERVER_DIR, &quot;scaling.json&quot;) json.dump({ **y_scaler.serialize(&quot;price&quot;), **y2_scaler.serialize(&quot;price_adj&quot;), **X_scaler.serialize(X.columns.str.lower()) }, open(SCALING_PATH,&quot;w&quot;), indent=2) . disp = [ [&quot;full dataset&quot;, f&quot;{len(X):,} rows&quot;, &quot;-&quot;], [&quot;training set&quot;, f&quot;{len(tr_idx):,} rows&quot;, f&quot;{len(tr_idx)/len(X):.2%}&quot;], [&quot;calibration set&quot;, f&quot;{len(cal_idx):,} rows&quot;, f&quot;{len(cal_idx)/len(X):.2%}&quot;], [&quot;validation set&quot;, f&quot;{len(val_idx):,} rows&quot;, f&quot;{len(val_idx)/len(X):.2%}&quot;] ] for a in disp: for b in a: print(b.ljust(20), end=&quot;&quot;) print() . full dataset 4,903,637 rows - training set 4,413,273 rows 90.00% calibration set 245,181 rows 5.00% validation set 245,183 rows 5.00% . Model selection . Model evaluation and helper functions . Mean Absolute Error. Lower is better. $$ textit{MAE}= frac {1}{n} sum_{i=1}^{n} left|y_i- hat{y}_i right|$$ | Mean Absolute Percentage Error. Lower is better. $$ textit{MAPE}= frac {100}{n} sum_{i=1}^{n} left| frac {y_i- hat{y}_i}{y_i} right|$$ | Root Mean Squared Error - error is in the same unit as the dependent variable. MSE &amp; RMSE are useful for checking the effect of outliers on the predictions. Lower is better. $$ textit{RMSE} = sqrt{ textit{mean of squares of residuals}} = sqrt{ frac {1}{n} sum _{i=1}^n(y_i-{ hat{y}_i})^2}$$ | RÂ² score - measures how well the predictors in the model explain the variability in the criterion (dependent variable). Best possible score is 1, higher is better. A negative score will reflect that the model is objectively worse at explaining the variability. $$R^2 = 1 - frac{ textit{sum of squares of residuals}}{ textit{sum of total squares}} = 1- frac{ sum_{i=1}^n(y_i- hat{y}_i)^2}{ sum_{i=1}^n({y_i - bar{y})^2}}$$ | . Where, $y$: expected value $ hat{y}$: predicted value $ bar{y}$: mean of expected values . def evaluate(y,yhat): resid = yhat-y mae = np.mean(np.abs(resid), axis=0) mape = 100*np.mean(np.abs(resid/y),axis=0) mse = np.mean(np.square(resid), axis=0) rmse = np.sqrt(mse) ssres = np.sum(resid**2) sstot = np.sum((y-np.mean(y, axis=0))**2) r2 = 1 - ssres/sstot return dict(mae=mae,mape=mape,rmse=rmse,r2=r2) . def plot_preds(y, yhat, interval=None, ax=None, npoints=50): # get first n points in sorted idx = np.argsort(y[:npoints]) # get min and max between of prediction vs expected for distance line vlines = np.array(list(zip(y,yhat))) vlines_min, vlines_max = vlines.min(axis=1), vlines.max(axis=1) if ax is None: ax = plt.subplots()[1] # plot prediction interval if provided if interval is not None: ax.vlines(range(npoints), interval[idx,0], interval[idx,1], &quot;green&quot;, alpha=.5, label=f&quot;{1-ALPHA:.1%} p.i. Â±{int(np.diff(interval[0])[0]//2):,}&quot;) ax.plot(interval[idx,0], &quot;g.&quot;) ax.plot(interval[idx,1], &quot;g.&quot;) ax.set_title(&quot;Predicted vs. Expected&quot;) ax.set_xlabel(&quot;sample no.&quot;) ax.set_ylabel(&quot;price&quot;) ax.plot(y[idx], &quot;b.&quot;, label=&quot;true&quot;) ax.plot(yhat[idx], &quot;r.&quot;, label=&quot;predicted&quot;) ax.vlines(range(npoints),vlines_min[idx],vlines_max[idx],&quot;black&quot;,&quot;--&quot;,alpha=.25,label=&quot;difference&quot;) ax.legend() return ax # Plotting feature importances def plot_imp(feat_imp, columns, horizontal=True, labels=True, pct=True, dp=1, ax=None): if ax is None: ax = plt.subplots()[1] if pct: feat_imp = [i/sum(feat_imp) * 100 for i in feat_imp] d = dict(zip(columns, feat_imp)) # making dictionary from values and keys d = {k: d[k] for k in sorted(d, key=d.get)} # sorted dictionary by values ax.set_title(&quot;Feature importances&quot;) if horizontal: ax.barh(list(d.keys()), list(d.values())) ax.tick_params(&quot;y&quot;, labelsize=&quot;small&quot;) else: ax.bar(list(d.keys()), list(d.values())) ax.tick_params(&quot;x&quot;, rotation=90, labelsize=&quot;small&quot;) if labels: ax.bar_label(ax.containers[0], labels=[f&quot;{i:.{dp}f}{&#39;%&#39; if pct else &#39;&#39;}&quot; for i in d.values()], label_type=&quot;edge&quot;, size=&quot;small&quot;) return ax . MEAN = df.PRICE.mean() resultStr = lambda key: f&quot;&quot;&quot;MAE ({results[key][&quot;mae&quot;]:,.0f}) is {results[key][&quot;mae&quot;]/MEAN:.2%} of mean RMSE ({results[key][&quot;rmse&quot;]:,.0f}) is {results[key][&quot;rmse&quot;]/MEAN:.2%} of mean MAPE {results[key][&quot;mape&quot;]:.2f}% R2 Score is {results[key][&quot;r2&quot;]:.3f}&quot;&quot;&quot; def Result(key, yhat, interval=None, feat_imp=None, y=inv_y, feature_names=X.columns, alpha=.05): results[key] = evaluate(y,yhat) print(resultStr(key)) _y = y[:yhat.shape[0]] f = plt.figure(figsize=(10,7), constrained_layout=False) gs = f.add_gridspec(nrows=6, ncols=2, hspace=2.75, wspace=.25) axs = [ f.add_subplot(gs[:2,0]), # residual histogram f.add_subplot(gs[2:,0]), # scatter plot f.add_subplot(gs[:3,1]), # pred vs expected with interval line f.add_subplot(gs[3:,1]) # feature importances ] f.suptitle(key) # Plot range of error sns.histplot(_y-yhat, kde=True, bins=50, ax=axs[0]) axs[0].set_title(&quot;Distribution of residuals&quot;) axs[0].set_xlabel(&quot;residual&quot;) axs[0].ticklabel_format(axis=&quot;x&quot;, style=&quot;sci&quot;, scilimits=(0,0)) # Scatter test against predictions axs[1].set_title(&quot;Predicted vs. Expected (scatter)&quot;) axs[1].scatter(_y,yhat,s=3, alpha=alpha) axs[1].set_ylabel(&quot;predicted&quot;) axs[1].set_xlabel(&quot;expected&quot;) plot_preds(_y, yhat, interval, ax=axs[2],npoints=35) # Importance barplot trunc = [col if len(col) &lt;= 10 else col[:7]+&quot;...&quot; for col in feature_names] if feat_imp is not None: plot_imp(feat_imp, trunc, ax=axs[3]) return axs . def permutation_importance(predict, X, y, repeats=5): baseline = np.mean([np.sqrt(np.mean(np.square(predict(X)-y), axis=0)) for i in range(repeats)]) results = [np.zeros(repeats)]*X.shape[1] for i in range(X.shape[1]): for n in range(repeats): X_copy = X.copy() X_copy[:,i] = np.random.permutation(X[:,i]) yhat = predict(X_copy) results[i][n] = np.sqrt(np.mean(np.square(yhat-y), axis=0)) del X_copy, yhat results[i] = results[i].mean() - baseline return np.array(results) . # hyperropt helper function from sklearn.model_selection import cross_val_score from hyperopt import STATUS_OK, STATUS_FAIL def objective(model, X, y, maximising=True, **cross_val_kwargs): # defaults for cross val score if &quot;scoring&quot; not in cross_val_kwargs: cross_val_kwargs[&quot;scoring&quot;] = &quot;neg_mean_squared_error&quot; if &quot;cv&quot; not in cross_val_kwargs: cross_val_kwargs[&quot;cv&quot;] = 3 if &quot;n_jobs&quot; not in cross_val_kwargs: cross_val_kwargs[&quot;n_jobs&quot;] = -1 # objective function returning score def func(params): reg = model(**params) loss = cross_val_score(reg, X, y, **cross_val_kwargs).mean() if maximising: # if score metric maximises (i.e higher = better) loss *= -1 # make loss negative for an objective to minimise return { &quot;loss&quot;: loss, &quot;status&quot;: STATUS_OK if not np.isnan(loss) else STATUS_FAIL } return func . Conformal Prediction . ALPHA = 0.15 class ConformalRegression(object): def __init__(self, model): self.model = model def calibrate(self, cal_X, cal_y): yhat = self.model.predict(cal_X) self.resid_ = np.abs(cal_y-yhat) return self def fit(self, X, y, cal_X, cal_y, **kwargs): self.model.fit(X, y, **kwargs) return self.calibrate(cal_X,cal_y) def predict(self, X, alpha = ALPHA): yhat = self.model.predict(X) if alpha is None: return yhat if alpha &gt;= 1 or alpha &lt;= 0: raise ValueError(&quot;&#39;alpha&#39; must be in interval (0, 1) or None&quot;) quantile = np.quantile(self.resid_, 1 - alpha) yhat_low = yhat - quantile yhat_up = yhat + quantile return yhat, np.column_stack([yhat_low, yhat_up]) . Multiple Linear Regression . Initally, I will begin by fitting a multiple linear regression model on my data. I expect this to perform poorly, as the linear regression will not capture the complex relationships and will assume linearity between the multiple predictor variables and the target variable. . Implementation . # algorithms/linear_regression/__init__.py class LinearRegression: @staticmethod def loss_fn(y,yhat): return np.mean(np.square(yhat-y)) def __init__(self, lr:float=.01): self.lr=lr self.history = None self.bias = 0 def predict(self,X:ndarray): return X.dot(self.weights) + self.bias # y = w.x + b def fit(self, X:ndarray, y:ndarray, val_X:ndarray=None, val_y:ndarray=None, epochs:int=1000, early_stopping:int=50, delta:float=0.01, verbose:bool=True ): X,y = np.asarray(X), np.asarray(y) self.weights = np.zeros(X.shape[1]) self.history = dict(loss=list()) if val_X is not None and val_y is not None: self.history[&quot;val_loss&quot;] = [] val_X, val_y = np.asarray(val_X), np.asarray(val_y) patience = None if isinstance(early_stopping, int) and early_stopping &gt; 0: patience = early_stopping best_loss = np.inf for i in (pbar:=tqdm(range(epochs), disable=not verbose)): if patience is not None and patience &lt;= 0: break yhat = self.predict(X) # cost gradient wrt params weight_grad = np.dot(X.T, yhat-y) / len(y) # gradient of loss wrt weights bias_grad = np.mean(yhat-y) # gradient of loss wrt bias # updating params (stochastic gradient descent) self.weights -= self.lr * weight_grad self.bias -= self.lr * bias_grad loss = self.loss_fn(y,yhat) self.history[&quot;loss&quot;].append(loss) # testing model on validation set if provided if &quot;val_loss&quot; in self.history: self.history[&quot;val_loss&quot;].append(self.loss_fn(val_y,self.predict(val_X))) # logging epoch info if enabled if verbose: msg = {&quot;loss&quot;: self.history[&quot;loss&quot;][-1]} if &quot;val_loss&quot; in self.history: msg[&quot;val_loss&quot;] = self.history[&quot;val_loss&quot;][-1] if patience is not None: msg[&quot;patience&quot;] = patience pbar.set_postfix(msg) # skip to next iteration if early stopping is disabled if patience is None: continue # if model has improved greater than minimum delta if best_loss - loss &gt;= delta: patience = early_stopping # reset patience best_loss = loss continue # skip to next iteration patience -= 1 # decrement patience (code wont be reached if model improved) return self . Fitting and evaluating . fn = &quot;models/LinearRegression.bin&quot; if not os.path.exists(fn): reg = LinearRegression().fit(tr_X, tr_y, val_X=val_X, val_y=val_y) dump(reg, fn) reg = ConformalRegression(load(fn)).calibrate(cal_X, cal_y) . f, ax = plt.subplots() ax.plot(reg.model.history[&quot;loss&quot;], &quot;--&quot;, label=&quot;Training loss&quot;) ax.plot(reg.model.history[&quot;val_loss&quot;], &quot;--&quot;, label=&quot;Validation loss&quot;) ax.set_title(&quot;Fitting linear regression model&quot;) ax.set_xlabel(&quot;epoch&quot;) ax.set_ylabel(&quot;loss (mse)&quot;) ax.legend(); . yhat, interval = [y_scaler.inv(y) for y in reg.predict(val_X)] Result(&quot;LinearRegression&quot;, yhat, interval, abs(reg.model.weights)); . MAE (78,682) is 35.49% of mean RMSE (117,887) is 53.18% of mean MAPE 47.35% R2 Score is 0.479 . The model performed better than expected, however it still wasn&#39;t great. We can begin to see which features have the most importance, such as the total floor area and location data. I will avoid other linear models, as they will likely perform similarly, and will instead try neighbour and tree-based methods instead. . K-Nearest Neighbour . I will now try the K-Nearest Neighbour algorithm to test if the distances between points, especially in their coordinates, can significantly impact the outcome price. The KNN algorithm works by using feature similarity to predict the values of new unseen data. A new point is assigned a value based on how closely it resembles other points in the training set, i.e the distance between them. The distance can be calculated using the Euclidean distance, or other methods such as the Manhattan, or Chebyzchev, distance. It then sorts the points and aggregrates the closest points for the outcome. This could be majority voting for classification, or the neighbourhood average for regression. The model is very sensitive to the curse of dimensionality, where adding more features doesn&#39;t benefit the model, rather it adds noise and hinders its performance. . Implementation . I will implement a custom model that utilises cuML&#39;s nearest neighbour search that will return the prediction as the inverse distance weighted average of the nearest neighbours, which will favour closer points. $$ bar x = frac{ sum_{i=1}^n w_i x_i}{ sum_{i=1}^nw_i} textrm{ where } w_i = frac{1}{d_{ix}^p} $$ To avoid zero division errors, I will only compute the weights of non-zero rows and will fill any remaining rows with a weights of 1 for zero distance, and 0 for all other distances. Since a distance of zero means an output sample is identical to the user&#39;s input, setting all other weights to 0 will solely favour the zero-distance sample for the output. To illustrate: . distance matrix = [[0.1, 0.4, 1.2, 2.1], [0.0, 0.2, 1.3, 1.9], [0.0, 0.1, 0.2, 0.1], [0.5, 0.6, 0.9, 1.1]] weights(distance matrix) = [[100, 6.25, 0.69, 0.22], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [4.0, 2.78, 1.23, 0.83]] . # algorithms/knn/__init__.py class KNNRegressor(object): def __init__(self, n_neighbours:int=5, idw_power:int=1): self.n_neighbours_ = n_neighbours self.idw_power_ = idw_power def fit(self, X, y, **kwargs): self.fit_X_ = X self.fit_y_ = y self.nearest_neigh_ = NearestNeighbors(**kwargs) self.nearest_neigh_.fit(X) return self def weights_fn(self, dist): weights = np.empty_like(dist) # masking all non-zero distance samples non_zero = np.all(dist!=0, axis=1) # setting non-zero weights to 1/dist^p weights[non_zero] = 1 / np.power(dist[non_zero], self.idw_power_ ) # binary mapping zero distance to 1 and 0 for all other. weights[~non_zero] = (dist[~non_zero] == 0).astype(float) return weights def predict(self, X): # getting k nearest neighbours dist, idx = self.nearest_neigh_.kneighbors(X, self.n_neighbours_) return np.average(self.fit_y_[idx], axis=1, weights=self.weights_fn(dist)) . Fitting and evaluating . fn = &quot;models/KNearestNeigh.bin&quot; if not os.path.exists(fn): reg = KNNRegressor(n_neighbours=5).fit(tr_X[:,1:], tr_y2) # omitting HPI column as not informative for stationary adjusted prices dump(reg, fn) reg = ConformalRegression(load(fn)).calibrate(cal_X[:,1:], cal_y2) . feat_imp = np.array([0.37192433, 0.28135066, 0.17182901, 0.0468285, 0.10096733]) # permutation_importance(lambda x: reg.predict(x, alpha=None), val_X[:25_000, 1:], val_y[:25_000]) . yhat, interval = [y2_scaler.inv(y) * LATEST_INDEX for y in reg.predict(val_X[:,1:])] Result(&quot;KNearestNeighbours(ADJ)&quot;, yhat, interval, feat_imp, inv_y2, X.columns[1:]); . MAE (63,421) is 28.61% of mean RMSE (103,374) is 46.63% of mean MAPE 21.28% R2 Score is 0.767 . Decision Tree regression . Now, I will attempt to implement a regression tree. I expect that the model will not perform well on its own, and I will probably need to use an ensemble method for best performance. My implementation will probably also be inefficient and will take a considerable amount of time in training, as I will be using recursion and expensive computations to build the tree. Additionally, I will suffer Python&#39;s various bottlenecks and overhead, where better, more established implementations use C++ instead. . Implementation . # algorithms/decision_tree/base.py class BaseNode(object): def predict(self,X:ndarray): raise NotImplementedError() def numeric(x, name, *, dtype=(float,int), min=float(&quot;-inf&quot;), max=float(&quot;inf&quot;), min_closed=True, max_closed=True): if not isinstance(x, dtype): type_ = dtype.__name__ if hasattr(dtype,&quot;__name__&quot;) else [dt.__name__ for dt in dtype] raise TypeError(f&quot;&#39;{name}&#39; parameter must be of type {type_}&quot;) interval = { (True,True): lambda x: min &lt;= x &lt;= max, # if closed interval (True,False): lambda x: min &lt;= x &lt; max, # if min closed, max open (False,True): lambda x: min &lt; x &lt;= max, # if min open, max closed (False,False): lambda x: min &lt; x &lt; max, # if open interval } mibr = &quot;[&quot; if min_closed else &quot;(&quot; mabr = &quot;]&quot; if max_closed else &quot;)&quot; if not interval[(min_closed, max_closed)](x): raise ValueError(f&quot;&#39;{name}&#39; parameter must be in interval {mibr}{min},{max}{mabr}&quot;) return x AggregateFunction = Callable[[ndarray],float] CriterionFunction = Callable[[ndarray],ndarray] ThreshSelectFunction = Callable[[ndarray],ndarray] . # algorithms/decision_tree/node.py class DecisionNode(BaseNode): def __init__(self,feature_idx,threshold,left,right): self.feature_idx = feature_idx self.threshold = threshold self.left = left self.right = right class LeafNode(BaseNode): def __init__(self,data): self.data = data def predict(self,X:ndarray): # finally reach leaf, so return data return self.data . # algorithms/decision_tree/__init__.py from collections import namedtuple Node = namedtuple(&quot;Node&quot;, &quot;feature_idx threshold left right data&quot;, defaults=[None]*5) class DecisionTree(object): # ===== aggregation functions ===== # == regression == @staticmethod def mean_aggr(y): # aggregating predictions by mean return np.mean(y) # == classification == @staticmethod def majority_aggr(y): # returning class with majority prediction vals, counts = np.unique(y, return_counts=True) return vals[np.argmax(counts)] # ===== criterion functions ===== # == regression == @staticmethod def variance(y): return np.sum(np.square(y - y.mean())) / len(y) # == classification == @staticmethod def _calc_proba(y): return np.unique(y,return_counts=True)[1] / len(y) @staticmethod def gini(y): proba = DecisionTree._calc_proba(y) return 1 - np.sum(np.square(proba)) @staticmethod def entropy(y): proba = DecisionTree._calc_proba(y) return - np.sum(proba*np.log2(proba)) # ===== impurity reduction / information gain ===== def information_gain(self, parent, left, right) -&gt; float: # information gain = (parent impurity) - (average children impurity) parent_impurity = self.criterion(parent) left_impurity = self.criterion(left) * len(left) right_impurity = self.criterion(right) * len(right) return parent_impurity - (left_impurity+right_impurity)/len(parent) reg_type = {&quot;reg&quot;, &quot;regression&quot;, &quot;regressor&quot;} cls_type = {&quot;cls&quot;, &quot;classification&quot;, &quot;classifier&quot;} def __init__(self, max_depth:int=5, max_features:float=1., min_samples_split:int=2, min_samples_leaf:int=1, min_split_gain:float=0, select_thresholds:ThreshSelectFunction=&quot;default&quot;, aggregate:Union[str,AggregateFunction]=&quot;regression&quot;, criterion:Union[str,CriterionFunction]=&quot;variance&quot;, seed=None ): self.random_state = np.random.RandomState(seed) self.root = None self.max_depth = numeric(max_depth, &quot;max_depth&quot;, dtype=int, min=3) self.max_features = numeric(max_features, &quot;max_features&quot;, dtype=float, min=0, max=1, min_closed=False) self.min_samples_leaf = numeric(min_samples_leaf, &quot;min_samples_leaf&quot;, dtype=int, min=1) self.min_samples_split = numeric(min_samples_split, &quot;min_samples_split&quot;, dtype=int) self.min_split_gain = numeric(min_split_gain, &quot;min_split_gain&quot;, min=0.) # threshold selection function validation sel_options = { &quot;default&quot;: lambda x: x # no special threshold selection } self.select_thresholds = sel_options.get(select_thresholds, select_thresholds) if not callable(self.select_thresholds): raise TypeError(f&quot;&#39;select_thresholds&#39; parameter must be in {set(sel_options.keys())} or &#39;callable(ndarray) -&gt; ndarray&#39;&quot;) # aggregate function validation aggr_options = { **{k:self.mean_aggr for k in self.reg_type}, # mean aggregation for regression **{k:self.majority_aggr for k in self.cls_type} # majority aggregation for classification } self.aggregate = aggr_options.get(aggregate, aggregate) if not callable(self.aggregate): raise TypeError(f&quot;&#39;aggregate&#39; parameter must be in {self.reg_type | self.cls_type} or &#39;callable(ndarray) -&gt; float&#39;&quot;) # criterion function validation criterion_options = { &quot;gini&quot;:self.gini, &quot;entropy&quot;:self.entropy, &quot;variance&quot;:self.variance, } self.criterion = criterion_options.get(criterion,criterion) if not callable(self.criterion): raise TypeError(f&quot;&#39;criterion&#39; parameter must be in {set(criterion_options.keys())} or &#39;callable(ndarray) -&gt; ndarray&#39;&quot;) def build_tree(self, X, y, depth=0): n_samples, n_features = X.shape if n_samples &gt; self.min_samples_leaf and depth &lt;= self.max_depth: feature_idx, threshold, left, right, gain = self.get_best_split(X, y, n_samples, n_features) # finding best split # splitting if information gain greater than some parameter (default 0) if gain &gt; self.min_split_gain: left_subtree = self.build_tree(X[left], y[left], depth + 1) right_subtree = self.build_tree(X[right], y[right], depth + 1) return Node(feature_idx, threshold, left_subtree, right_subtree) # leaf node if max depth reached, minimum samples in leaf, minimum samples in split # aggregrating sample points into single point leaf_data = self.aggregate(y) return Node(data=leaf_data) def get_best_split(self, X, y, n_samples, n_features): split = None idx = np.arange(n_samples) best = float(&quot;-inf&quot;) # selecting features randomly sel_features = self.random_state.permutation(n_features)[:int(round(n_features*self.max_features))] # iterating through features for feature_idx in sel_features: feature = X[:,feature_idx] # getting current column of dataset thresh_potential = self.select_thresholds(np.unique(feature)) # selecting potential thresholds # iterating through thresholds to find best feature-threshold combination for threshold in thresh_potential: left = idx[feature &lt;= threshold] # getting indices of data points less than threshold for left subtree right = idx[feature &gt; threshold] # getting indices of data points greater than threshold for right subtree # if minimal data after split skip execution and begin next iteration if len(left) &lt;= self.min_samples_split or len(right) &lt;= self.min_samples_split: continue # calculating quality of split gain = self.information_gain(y, y[left], y[right]) if gain &gt;= best: split = ( feature_idx, threshold, left, right, gain ) best = gain return split def fit(self,X,y): self.root = self.build_tree(X,y) return self def _predict(self,X,node): # if leaf node, return data if node.data is not None: return node.data # recursively traversing down tree val = X[node.feature_idx] if val &lt;= node.threshold: # traverse left if value of feature in X less than threshold return self._predict(X,node.left) else: # otherwise traverse right return self._predict(X,node.right) def predict(self,X): return np.array([self._predict(x,self.root) for x in X]) . Fitting and evaluating . def cluster_thresholds(array, diff, aggr=np.mean): # aggregrating similar thresholds to reduce number of potential splits tmp = array.copy() groups = [] while len(tmp): # select seed seed = tmp.min() mask = (tmp - seed) &lt;= diff groups.append(aggr(tmp[mask, None])) tmp = tmp[~mask] return groups . from functools import partial fn = &quot;models/DecisionTree.bin&quot; if not os.path.exists(fn): reg = DecisionTree( max_depth=7, # stop growing at depth 7 min_samples_leaf=512, # to speed up training aggregate=&quot;regression&quot;, # mean aggregrate samples at leaf node criterion=&quot;variance&quot;, # use variance reduction # cluster and aggregrate thresholds with 0.005 difference between them, categorical features should stay unaffected select_thresholds=partial(cluster_thresholds, diff=0.005), seed=SEED ).fit(tr_X[:,1:], tr_y2) dump(reg, fn) reg = ConformalRegression(load(fn)).calibrate(cal_X[:,1:], cal_y2) . feat_imp = permutation_importance(lambda x: reg.predict(x, alpha=None), val_X[:,1:], val_y2) . yhat, interval = [y2_scaler.inv(y) * LATEST_INDEX for y in reg.predict(val_X[:,1:])] Result(&quot;DecisionTree(ADJ)&quot;, yhat, interval, feat_imp, inv_y2, X.columns[1:]); . MAE (81,266) is 36.66% of mean RMSE (125,512) is 56.62% of mean MAPE 28.47% R2 Score is 0.657 . As seen above, the model produces discrete predictions based on the leaf nodes of the fitted tree. I believe increasing the maximum depth (thereby increasing the number of leaves) would help improve the bands of predictions that can be seen on the scatter diagram. Despite this issue, I believe using an ensemble method to combine multiple trees will greatly reduce the issues I have described. . Random Forest . Hyperparameter optimisation . I will begin by optimising the random forest&#39;s parameters on a small set of data, with a low number of estimators, for quick and effective computation. The models will be optimised to minimise the RMSE (root mean squared error) of the predictions. I will be running the optimisation over 100 iterations. I will be using the hyperopt library which will make this process rapid and reliable. . pbounds = { &quot;max_depth&quot;: hp.uniformint(&quot;max_depth&quot;, 5, 20), # maximum depth to grow trees to, too high/low could result in overfitting/underfitting &quot;max_features&quot;: hp.uniform(&quot;max_features&quot;, 0.5, 1.0), # maximum number of features to consider when making splits &quot;min_samples_split&quot;: hp.uniformint(&quot;min_samples_split&quot;, 1, 64), # minimum number of samples required to split &quot;min_samples_leaf&quot;: hp.uniformint(&quot;min_samples_leaf&quot;, 1, 64), # minimum number of samples in each leaf node # static parameters &quot;n_bins&quot;:256, &quot;n_estimators&quot;: 50, # number of trees to build. low number for now, will increase when fitting resulting model &quot;random_state&quot;: SEED # for reproducibility, may still have some randomness due to n_streams &gt; 1. } n = 100_000 idx = np.random.permutation(len(tr_X))[:n] sample_x, sample_y = tr_X[idx,1:].astype(np.float32), tr_y2[idx].astype(np.float32) . fn = &quot;models/_rf_trials.hyperopt&quot; trials = Trials() if not os.path.exists(fn) else load(fn) result = fmin( fn = objective( model = RandomForestRegressor, X = sample_x, y = sample_y, cv = 5, ), space = pbounds, algo = tpe.suggest, max_evals = 100, trials = trials ) if not os.path.exists(fn): dump(trials, fn) rfparams = {k:v[0] for k,v in trials.best_trial[&quot;misc&quot;][&quot;vals&quot;].items()} rfparams[&quot;max_depth&quot;] = int(rfparams[&quot;max_depth&quot;]) rfparams[&quot;min_samples_split&quot;] = int(rfparams[&quot;min_samples_split&quot;]) rfparams[&quot;min_samples_leaf&quot;] = int(rfparams[&quot;min_samples_leaf&quot;]) . 100%|ââââââââââ| 100/100 [00:00&lt;?, ?trial/s, best loss=?] . f, axs = plt.subplots(2, (len(trials.vals)+1)//2, sharey=True, figsize=(10,5)) axs[0,0].set_ylabel(&quot;rmse&quot;) axs[1,0].set_ylabel(&quot;rmse&quot;) for k, ax in zip(trials.vals, axs.flatten()): ax.scatter(trials.vals[k], trials.losses(), 5) ax.set_xlabel(k) print(f&quot;&quot;&quot;best loss: {trials.best_trial[&quot;result&quot;][&quot;loss&quot;]}, params: {pformat(rfparams)}&quot;&quot;&quot;) f.tight_layout() . best loss: 0.2328524947166443, params: {&#39;max_depth&#39;: 20, &#39;max_features&#39;: 0.9965555438979297, &#39;min_samples_leaf&#39;: 6, &#39;min_samples_split&#39;: 7} . Fitting and evaluating . fn = &quot;models/RandomForest.bin&quot; if not os.path.exists(fn): # gpu accelerated training rf = RandomForestRegressor( **rfparams, n_bins = 256, n_estimators = 100, ).fit(tr_X[:,1:].astype(np.float32), tr_y2.astype(np.float32)) # serialising as treelite model to make available # for cpu inference and cross-platform tl_model = rf.convert_to_treelite_model() del rf tl_model.to_treelite_checkpoint(fn) else: tl_model = treelite.Model.deserialize(fn) reg = ConformalRegression(ForestInference().load_from_treelite_model(tl_model, output_class=False)).calibrate(cal_X[:,1:], cal_y2) . feat_imp = permutation_importance(lambda x: reg.predict(x, alpha=None), val_X[:,1:], val_y) . yhat, interval = [y2_scaler.inv(y) * LATEST_INDEX for y in reg.predict(val_X[:,1:])] Result(&quot;RandomForest(ADJ)&quot;, yhat, interval, feat_imp, inv_y2, X.columns[1:]); . MAE (55,801) is 25.17% of mean RMSE (90,258) is 40.71% of mean MAPE 19.38% R2 Score is 0.823 . I am pleased with the performance of the random forest as it is not only versatile, but also extremely fast, as a result of the treelite compiler and the gpu-accelerated ForestInference front-end. I will now test gradient boosting machines (XGBoost) accelerated with treelite and ForestInference, to test if it can also perform with similar efficiency. . Gradient Boosting Machine . Hyperparameter Optimisation . Similar to the random forest, I will begin by optimising the parameters of the gradient boosting machine using hyperopt. . pbounds = { &quot;learning_rate&quot;: hp.uniform(&quot;learning_rate&quot;, 0.01, 0.25), &quot;max_depth&quot;: hp.uniformint(&quot;max_depth&quot;, 5, 20), # maximum depth to grow trees to, too high/low could result in overfitting/underfitting &quot;colsample_bynode&quot;: hp.uniform(&quot;colsample_bynode&quot;, 0.5, 1.0), # maximum number of features to consider when making splits &quot;min_child_weight&quot;: hp.uniformint(&quot;min_child_weight&quot;, 1, 32), # minimum number of samples in each leaf node # static parameters &quot;random_state&quot;: SEED, &quot;n_estimators&quot;: 50, &quot;tree_method&quot;: &quot;gpu_hist&quot; # gpu acceleration } n = 50_000 idx = np.random.permutation(len(tr_X))[:n] sample_x, sample_y = tr_X[idx, 1:].astype(np.float32), tr_y2[idx].astype(np.float32) . fn = &quot;models/_xgb_trials.hyperopt&quot; trials = Trials() if not os.path.exists(fn) else load(fn) result = fmin( fn = objective( model = xgb.XGBRegressor, X = sample_x, y = sample_y, cv = 2, ), space = pbounds, algo = tpe.suggest, max_evals = 100, trials = trials ) if not os.path.exists(fn): dump(trials, fn) xgbparams = {k:v[0] for k,v in trials.best_trial[&quot;misc&quot;][&quot;vals&quot;].items()} xgbparams[&quot;max_depth&quot;] = int(round(xgbparams[&quot;max_depth&quot;])) xgbparams[&quot;min_child_weight&quot;] = int(round(xgbparams[&quot;min_child_weight&quot;])) . 100%|ââââââââââ| 100/100 [00:00&lt;?, ?trial/s, best loss=?] . f, axs = plt.subplots(2, (len(trials.vals)+1)//2, sharey=True, figsize=(10,5)) axs[0,0].set_ylabel(&quot;rmse&quot;) axs[1,0].set_ylabel(&quot;rmse&quot;) for k, ax in zip(trials.vals, axs.flatten()): ax.scatter(trials.vals[k], trials.losses(), 5) ax.set_xlabel(k) print(f&quot;&quot;&quot;best loss: {trials.best_trial[&quot;result&quot;][&quot;loss&quot;]}, params: {pformat(xgbparams)}&quot;&quot;&quot;) f.tight_layout() . best loss: 0.22684506326913834, params: {&#39;colsample_bynode&#39;: 0.8708868938454073, &#39;learning_rate&#39;: 0.2088532944217484, &#39;max_depth&#39;: 11, &#39;min_child_weight&#39;: 17} . Fitting and evaluating . fn = &quot;models/XGBoost.bin&quot; if not os.path.exists(fn): dtrain = xgb.DMatrix(tr_X[:,1:], tr_y2) dtest = xgb.DMatrix(val_X[:,1:], val_y2) xgbparams[&quot;random_state&quot;] = SEED xgbparams[&quot;tree_method&quot;] = &quot;gpu_hist&quot; xgb_ = xgb.train(xgbparams, dtrain, evals=[(dtrain, &quot;train&quot;), (dtest, &quot;val&quot;)], num_boost_round=2000, early_stopping_rounds=10) xgb_.save_model(fn) else: xgb_ = xgb.Booster() xgb_.load_model(fn) reg = ConformalRegression(ForestInference().load(fn, output_class=False)).calibrate(cal_X[:,1:], cal_y2) . [0] train-rmse:0.94074 val-rmse:0.94086 [1] train-rmse:0.80932 val-rmse:0.80955 [2] train-rmse:0.71279 val-rmse:0.71322 [3] train-rmse:0.64249 val-rmse:0.64307 [4] train-rmse:0.59205 val-rmse:0.59300 [5] train-rmse:0.55624 val-rmse:0.55738 [6] train-rmse:0.53050 val-rmse:0.53204 [7] train-rmse:0.51153 val-rmse:0.51330 [8] train-rmse:0.49585 val-rmse:0.49798 [9] train-rmse:0.48542 val-rmse:0.48783 [10] train-rmse:0.47730 val-rmse:0.48009 [11] train-rmse:0.47099 val-rmse:0.47413 [12] train-rmse:0.46541 val-rmse:0.46888 [13] train-rmse:0.46154 val-rmse:0.46531 [14] train-rmse:0.45853 val-rmse:0.46251 [15] train-rmse:0.45556 val-rmse:0.45971 [16] train-rmse:0.45246 val-rmse:0.45687 [17] train-rmse:0.45079 val-rmse:0.45544 [18] train-rmse:0.44841 val-rmse:0.45332 [19] train-rmse:0.44641 val-rmse:0.45162 [20] train-rmse:0.44471 val-rmse:0.45016 [21] train-rmse:0.44286 val-rmse:0.44845 [22] train-rmse:0.44200 val-rmse:0.44776 [23] train-rmse:0.44039 val-rmse:0.44633 [24] train-rmse:0.43977 val-rmse:0.44586 [25] train-rmse:0.43856 val-rmse:0.44479 [26] train-rmse:0.43764 val-rmse:0.44394 [27] train-rmse:0.43730 val-rmse:0.44375 [28] train-rmse:0.43629 val-rmse:0.44296 [29] train-rmse:0.43492 val-rmse:0.44172 [30] train-rmse:0.43407 val-rmse:0.44097 [31] train-rmse:0.43358 val-rmse:0.44059 [32] train-rmse:0.43288 val-rmse:0.44005 [33] train-rmse:0.43223 val-rmse:0.43955 [34] train-rmse:0.43201 val-rmse:0.43939 [35] train-rmse:0.43161 val-rmse:0.43905 [36] train-rmse:0.43124 val-rmse:0.43879 [37] train-rmse:0.43095 val-rmse:0.43866 [38] train-rmse:0.43027 val-rmse:0.43801 [39] train-rmse:0.42911 val-rmse:0.43702 [40] train-rmse:0.42865 val-rmse:0.43666 [41] train-rmse:0.42832 val-rmse:0.43637 [42] train-rmse:0.42768 val-rmse:0.43588 [43] train-rmse:0.42723 val-rmse:0.43559 [44] train-rmse:0.42697 val-rmse:0.43547 [45] train-rmse:0.42635 val-rmse:0.43496 [46] train-rmse:0.42604 val-rmse:0.43476 [47] train-rmse:0.42532 val-rmse:0.43420 [48] train-rmse:0.42437 val-rmse:0.43340 [49] train-rmse:0.42393 val-rmse:0.43305 [50] train-rmse:0.42345 val-rmse:0.43271 [51] train-rmse:0.42271 val-rmse:0.43220 [52] train-rmse:0.42235 val-rmse:0.43197 [53] train-rmse:0.42203 val-rmse:0.43175 [54] train-rmse:0.42174 val-rmse:0.43158 [55] train-rmse:0.42121 val-rmse:0.43119 [56] train-rmse:0.42069 val-rmse:0.43075 [57] train-rmse:0.42031 val-rmse:0.43044 [58] train-rmse:0.41985 val-rmse:0.43013 [59] train-rmse:0.41958 val-rmse:0.42995 [60] train-rmse:0.41934 val-rmse:0.42978 [61] train-rmse:0.41897 val-rmse:0.42950 [62] train-rmse:0.41861 val-rmse:0.42920 [63] train-rmse:0.41829 val-rmse:0.42908 [64] train-rmse:0.41801 val-rmse:0.42893 [65] train-rmse:0.41736 val-rmse:0.42834 [66] train-rmse:0.41632 val-rmse:0.42745 [67] train-rmse:0.41594 val-rmse:0.42718 [68] train-rmse:0.41561 val-rmse:0.42694 [69] train-rmse:0.41537 val-rmse:0.42685 [70] train-rmse:0.41504 val-rmse:0.42663 [71] train-rmse:0.41487 val-rmse:0.42656 [72] train-rmse:0.41470 val-rmse:0.42648 [73] train-rmse:0.41430 val-rmse:0.42630 [74] train-rmse:0.41386 val-rmse:0.42604 [75] train-rmse:0.41367 val-rmse:0.42594 [76] train-rmse:0.41335 val-rmse:0.42572 [77] train-rmse:0.41319 val-rmse:0.42567 [78] train-rmse:0.41285 val-rmse:0.42538 [79] train-rmse:0.41214 val-rmse:0.42483 [80] train-rmse:0.41187 val-rmse:0.42468 [81] train-rmse:0.41168 val-rmse:0.42460 [82] train-rmse:0.41156 val-rmse:0.42454 [83] train-rmse:0.41142 val-rmse:0.42450 [84] train-rmse:0.41124 val-rmse:0.42439 [85] train-rmse:0.41105 val-rmse:0.42427 [86] train-rmse:0.41045 val-rmse:0.42384 [87] train-rmse:0.41011 val-rmse:0.42360 [88] train-rmse:0.41005 val-rmse:0.42356 [89] train-rmse:0.40992 val-rmse:0.42351 [90] train-rmse:0.40960 val-rmse:0.42322 [91] train-rmse:0.40938 val-rmse:0.42310 [92] train-rmse:0.40905 val-rmse:0.42286 [93] train-rmse:0.40893 val-rmse:0.42285 [94] train-rmse:0.40834 val-rmse:0.42237 [95] train-rmse:0.40818 val-rmse:0.42231 [96] train-rmse:0.40799 val-rmse:0.42221 [97] train-rmse:0.40777 val-rmse:0.42211 [98] train-rmse:0.40759 val-rmse:0.42201 [99] train-rmse:0.40749 val-rmse:0.42197 [100] train-rmse:0.40739 val-rmse:0.42189 [101] train-rmse:0.40720 val-rmse:0.42177 [102] train-rmse:0.40690 val-rmse:0.42158 [103] train-rmse:0.40653 val-rmse:0.42128 [104] train-rmse:0.40642 val-rmse:0.42125 [105] train-rmse:0.40615 val-rmse:0.42111 [106] train-rmse:0.40599 val-rmse:0.42102 [107] train-rmse:0.40583 val-rmse:0.42096 [108] train-rmse:0.40554 val-rmse:0.42070 [109] train-rmse:0.40535 val-rmse:0.42063 [110] train-rmse:0.40521 val-rmse:0.42057 [111] train-rmse:0.40500 val-rmse:0.42040 [112] train-rmse:0.40482 val-rmse:0.42027 [113] train-rmse:0.40472 val-rmse:0.42019 [114] train-rmse:0.40453 val-rmse:0.42006 [115] train-rmse:0.40405 val-rmse:0.41970 [116] train-rmse:0.40375 val-rmse:0.41954 [117] train-rmse:0.40343 val-rmse:0.41933 [118] train-rmse:0.40320 val-rmse:0.41920 [119] train-rmse:0.40302 val-rmse:0.41908 [120] train-rmse:0.40293 val-rmse:0.41906 [121] train-rmse:0.40284 val-rmse:0.41897 [122] train-rmse:0.40275 val-rmse:0.41892 [123] train-rmse:0.40261 val-rmse:0.41890 [124] train-rmse:0.40247 val-rmse:0.41887 [125] train-rmse:0.40229 val-rmse:0.41874 [126] train-rmse:0.40214 val-rmse:0.41865 [127] train-rmse:0.40205 val-rmse:0.41862 [128] train-rmse:0.40173 val-rmse:0.41837 [129] train-rmse:0.40166 val-rmse:0.41833 [130] train-rmse:0.40139 val-rmse:0.41820 [131] train-rmse:0.40113 val-rmse:0.41803 [132] train-rmse:0.40107 val-rmse:0.41801 [133] train-rmse:0.40103 val-rmse:0.41799 [134] train-rmse:0.40088 val-rmse:0.41789 [135] train-rmse:0.40075 val-rmse:0.41783 [136] train-rmse:0.40065 val-rmse:0.41777 [137] train-rmse:0.40054 val-rmse:0.41774 [138] train-rmse:0.40044 val-rmse:0.41772 [139] train-rmse:0.40034 val-rmse:0.41765 [140] train-rmse:0.40020 val-rmse:0.41758 [141] train-rmse:0.40002 val-rmse:0.41747 [142] train-rmse:0.39998 val-rmse:0.41746 [143] train-rmse:0.39984 val-rmse:0.41741 [144] train-rmse:0.39962 val-rmse:0.41732 [145] train-rmse:0.39951 val-rmse:0.41731 [146] train-rmse:0.39942 val-rmse:0.41730 [147] train-rmse:0.39933 val-rmse:0.41725 [148] train-rmse:0.39919 val-rmse:0.41723 [149] train-rmse:0.39910 val-rmse:0.41720 [150] train-rmse:0.39893 val-rmse:0.41708 [151] train-rmse:0.39884 val-rmse:0.41704 [152] train-rmse:0.39872 val-rmse:0.41697 [153] train-rmse:0.39847 val-rmse:0.41683 [154] train-rmse:0.39831 val-rmse:0.41672 [155] train-rmse:0.39816 val-rmse:0.41664 [156] train-rmse:0.39790 val-rmse:0.41651 [157] train-rmse:0.39781 val-rmse:0.41649 [158] train-rmse:0.39767 val-rmse:0.41643 [159] train-rmse:0.39763 val-rmse:0.41643 [160] train-rmse:0.39751 val-rmse:0.41637 [161] train-rmse:0.39744 val-rmse:0.41637 [162] train-rmse:0.39731 val-rmse:0.41628 [163] train-rmse:0.39722 val-rmse:0.41627 [164] train-rmse:0.39709 val-rmse:0.41618 [165] train-rmse:0.39698 val-rmse:0.41611 [166] train-rmse:0.39686 val-rmse:0.41608 [167] train-rmse:0.39664 val-rmse:0.41591 [168] train-rmse:0.39655 val-rmse:0.41587 [169] train-rmse:0.39630 val-rmse:0.41570 [170] train-rmse:0.39621 val-rmse:0.41565 [171] train-rmse:0.39615 val-rmse:0.41563 [172] train-rmse:0.39600 val-rmse:0.41553 [173] train-rmse:0.39593 val-rmse:0.41550 [174] train-rmse:0.39582 val-rmse:0.41548 [175] train-rmse:0.39575 val-rmse:0.41543 [176] train-rmse:0.39569 val-rmse:0.41539 [177] train-rmse:0.39553 val-rmse:0.41534 [178] train-rmse:0.39543 val-rmse:0.41527 [179] train-rmse:0.39524 val-rmse:0.41521 [180] train-rmse:0.39515 val-rmse:0.41520 [181] train-rmse:0.39501 val-rmse:0.41510 [182] train-rmse:0.39497 val-rmse:0.41508 [183] train-rmse:0.39477 val-rmse:0.41501 [184] train-rmse:0.39467 val-rmse:0.41498 [185] train-rmse:0.39460 val-rmse:0.41493 [186] train-rmse:0.39451 val-rmse:0.41491 [187] train-rmse:0.39430 val-rmse:0.41476 [188] train-rmse:0.39419 val-rmse:0.41473 [189] train-rmse:0.39409 val-rmse:0.41466 [190] train-rmse:0.39401 val-rmse:0.41465 [191] train-rmse:0.39395 val-rmse:0.41464 [192] train-rmse:0.39368 val-rmse:0.41450 [193] train-rmse:0.39338 val-rmse:0.41431 [194] train-rmse:0.39318 val-rmse:0.41417 [195] train-rmse:0.39314 val-rmse:0.41417 [196] train-rmse:0.39305 val-rmse:0.41414 [197] train-rmse:0.39278 val-rmse:0.41399 [198] train-rmse:0.39261 val-rmse:0.41394 [199] train-rmse:0.39250 val-rmse:0.41391 [200] train-rmse:0.39241 val-rmse:0.41387 [201] train-rmse:0.39234 val-rmse:0.41384 [202] train-rmse:0.39228 val-rmse:0.41381 [203] train-rmse:0.39220 val-rmse:0.41379 [204] train-rmse:0.39212 val-rmse:0.41378 [205] train-rmse:0.39208 val-rmse:0.41377 [206] train-rmse:0.39200 val-rmse:0.41374 [207] train-rmse:0.39195 val-rmse:0.41373 [208] train-rmse:0.39178 val-rmse:0.41366 [209] train-rmse:0.39169 val-rmse:0.41363 [210] train-rmse:0.39161 val-rmse:0.41359 [211] train-rmse:0.39155 val-rmse:0.41357 [212] train-rmse:0.39147 val-rmse:0.41353 [213] train-rmse:0.39134 val-rmse:0.41355 [214] train-rmse:0.39125 val-rmse:0.41351 [215] train-rmse:0.39111 val-rmse:0.41350 [216] train-rmse:0.39099 val-rmse:0.41345 [217] train-rmse:0.39089 val-rmse:0.41344 [218] train-rmse:0.39083 val-rmse:0.41341 [219] train-rmse:0.39078 val-rmse:0.41341 [220] train-rmse:0.39065 val-rmse:0.41339 [221] train-rmse:0.39050 val-rmse:0.41329 [222] train-rmse:0.39043 val-rmse:0.41328 [223] train-rmse:0.39036 val-rmse:0.41326 [224] train-rmse:0.39025 val-rmse:0.41320 [225] train-rmse:0.39017 val-rmse:0.41321 [226] train-rmse:0.39014 val-rmse:0.41321 [227] train-rmse:0.39007 val-rmse:0.41318 [228] train-rmse:0.39002 val-rmse:0.41317 [229] train-rmse:0.38993 val-rmse:0.41317 [230] train-rmse:0.38989 val-rmse:0.41315 [231] train-rmse:0.38987 val-rmse:0.41316 [232] train-rmse:0.38982 val-rmse:0.41314 [233] train-rmse:0.38960 val-rmse:0.41303 [234] train-rmse:0.38949 val-rmse:0.41299 [235] train-rmse:0.38944 val-rmse:0.41298 [236] train-rmse:0.38939 val-rmse:0.41297 [237] train-rmse:0.38934 val-rmse:0.41295 [238] train-rmse:0.38928 val-rmse:0.41292 [239] train-rmse:0.38907 val-rmse:0.41282 [240] train-rmse:0.38900 val-rmse:0.41279 [241] train-rmse:0.38891 val-rmse:0.41278 [242] train-rmse:0.38888 val-rmse:0.41277 [243] train-rmse:0.38876 val-rmse:0.41275 [244] train-rmse:0.38850 val-rmse:0.41263 [245] train-rmse:0.38842 val-rmse:0.41260 [246] train-rmse:0.38825 val-rmse:0.41253 [247] train-rmse:0.38820 val-rmse:0.41252 [248] train-rmse:0.38814 val-rmse:0.41251 [249] train-rmse:0.38808 val-rmse:0.41251 [250] train-rmse:0.38801 val-rmse:0.41251 [251] train-rmse:0.38795 val-rmse:0.41250 [252] train-rmse:0.38785 val-rmse:0.41246 [253] train-rmse:0.38776 val-rmse:0.41241 [254] train-rmse:0.38770 val-rmse:0.41238 [255] train-rmse:0.38765 val-rmse:0.41237 [256] train-rmse:0.38754 val-rmse:0.41233 [257] train-rmse:0.38745 val-rmse:0.41230 [258] train-rmse:0.38734 val-rmse:0.41225 [259] train-rmse:0.38726 val-rmse:0.41223 [260] train-rmse:0.38722 val-rmse:0.41221 [261] train-rmse:0.38717 val-rmse:0.41221 [262] train-rmse:0.38705 val-rmse:0.41219 [263] train-rmse:0.38694 val-rmse:0.41215 [264] train-rmse:0.38689 val-rmse:0.41213 [265] train-rmse:0.38687 val-rmse:0.41212 [266] train-rmse:0.38679 val-rmse:0.41212 [267] train-rmse:0.38656 val-rmse:0.41205 [268] train-rmse:0.38652 val-rmse:0.41202 [269] train-rmse:0.38644 val-rmse:0.41202 [270] train-rmse:0.38640 val-rmse:0.41202 [271] train-rmse:0.38636 val-rmse:0.41199 [272] train-rmse:0.38628 val-rmse:0.41195 [273] train-rmse:0.38623 val-rmse:0.41193 [274] train-rmse:0.38617 val-rmse:0.41193 [275] train-rmse:0.38612 val-rmse:0.41192 [276] train-rmse:0.38604 val-rmse:0.41190 [277] train-rmse:0.38598 val-rmse:0.41185 [278] train-rmse:0.38592 val-rmse:0.41184 [279] train-rmse:0.38586 val-rmse:0.41184 [280] train-rmse:0.38579 val-rmse:0.41180 [281] train-rmse:0.38576 val-rmse:0.41180 [282] train-rmse:0.38571 val-rmse:0.41179 [283] train-rmse:0.38565 val-rmse:0.41178 [284] train-rmse:0.38561 val-rmse:0.41177 [285] train-rmse:0.38554 val-rmse:0.41176 [286] train-rmse:0.38549 val-rmse:0.41174 [287] train-rmse:0.38543 val-rmse:0.41173 [288] train-rmse:0.38541 val-rmse:0.41173 [289] train-rmse:0.38533 val-rmse:0.41171 [290] train-rmse:0.38530 val-rmse:0.41171 [291] train-rmse:0.38528 val-rmse:0.41171 [292] train-rmse:0.38510 val-rmse:0.41167 [293] train-rmse:0.38504 val-rmse:0.41166 [294] train-rmse:0.38502 val-rmse:0.41166 [295] train-rmse:0.38494 val-rmse:0.41162 [296] train-rmse:0.38482 val-rmse:0.41160 [297] train-rmse:0.38477 val-rmse:0.41159 [298] train-rmse:0.38470 val-rmse:0.41158 [299] train-rmse:0.38467 val-rmse:0.41158 [300] train-rmse:0.38464 val-rmse:0.41158 [301] train-rmse:0.38460 val-rmse:0.41155 [302] train-rmse:0.38449 val-rmse:0.41149 [303] train-rmse:0.38444 val-rmse:0.41148 [304] train-rmse:0.38440 val-rmse:0.41149 [305] train-rmse:0.38424 val-rmse:0.41145 [306] train-rmse:0.38414 val-rmse:0.41142 [307] train-rmse:0.38408 val-rmse:0.41140 [308] train-rmse:0.38405 val-rmse:0.41140 [309] train-rmse:0.38402 val-rmse:0.41140 [310] train-rmse:0.38395 val-rmse:0.41139 [311] train-rmse:0.38389 val-rmse:0.41136 [312] train-rmse:0.38370 val-rmse:0.41133 [313] train-rmse:0.38363 val-rmse:0.41131 [314] train-rmse:0.38355 val-rmse:0.41130 [315] train-rmse:0.38346 val-rmse:0.41124 [316] train-rmse:0.38338 val-rmse:0.41123 [317] train-rmse:0.38331 val-rmse:0.41121 [318] train-rmse:0.38325 val-rmse:0.41120 [319] train-rmse:0.38319 val-rmse:0.41119 [320] train-rmse:0.38312 val-rmse:0.41116 [321] train-rmse:0.38305 val-rmse:0.41113 [322] train-rmse:0.38291 val-rmse:0.41109 [323] train-rmse:0.38282 val-rmse:0.41109 [324] train-rmse:0.38278 val-rmse:0.41110 [325] train-rmse:0.38272 val-rmse:0.41110 [326] train-rmse:0.38269 val-rmse:0.41108 [327] train-rmse:0.38256 val-rmse:0.41109 [328] train-rmse:0.38253 val-rmse:0.41108 [329] train-rmse:0.38245 val-rmse:0.41103 [330] train-rmse:0.38242 val-rmse:0.41102 [331] train-rmse:0.38238 val-rmse:0.41102 [332] train-rmse:0.38229 val-rmse:0.41098 [333] train-rmse:0.38222 val-rmse:0.41097 [334] train-rmse:0.38214 val-rmse:0.41097 [335] train-rmse:0.38199 val-rmse:0.41091 [336] train-rmse:0.38194 val-rmse:0.41090 [337] train-rmse:0.38187 val-rmse:0.41089 [338] train-rmse:0.38181 val-rmse:0.41086 [339] train-rmse:0.38171 val-rmse:0.41086 [340] train-rmse:0.38163 val-rmse:0.41084 [341] train-rmse:0.38156 val-rmse:0.41084 [342] train-rmse:0.38147 val-rmse:0.41082 [343] train-rmse:0.38140 val-rmse:0.41082 [344] train-rmse:0.38138 val-rmse:0.41082 [345] train-rmse:0.38131 val-rmse:0.41080 [346] train-rmse:0.38128 val-rmse:0.41079 [347] train-rmse:0.38123 val-rmse:0.41079 [348] train-rmse:0.38119 val-rmse:0.41079 [349] train-rmse:0.38108 val-rmse:0.41075 [350] train-rmse:0.38104 val-rmse:0.41072 [351] train-rmse:0.38100 val-rmse:0.41069 [352] train-rmse:0.38095 val-rmse:0.41070 [353] train-rmse:0.38092 val-rmse:0.41070 [354] train-rmse:0.38090 val-rmse:0.41069 [355] train-rmse:0.38082 val-rmse:0.41070 [356] train-rmse:0.38079 val-rmse:0.41069 [357] train-rmse:0.38075 val-rmse:0.41069 [358] train-rmse:0.38074 val-rmse:0.41069 [359] train-rmse:0.38073 val-rmse:0.41069 [360] train-rmse:0.38064 val-rmse:0.41065 [361] train-rmse:0.38057 val-rmse:0.41065 [362] train-rmse:0.38054 val-rmse:0.41064 [363] train-rmse:0.38050 val-rmse:0.41064 [364] train-rmse:0.38036 val-rmse:0.41060 [365] train-rmse:0.38031 val-rmse:0.41059 [366] train-rmse:0.38026 val-rmse:0.41058 [367] train-rmse:0.38022 val-rmse:0.41057 [368] train-rmse:0.38016 val-rmse:0.41057 [369] train-rmse:0.38012 val-rmse:0.41056 [370] train-rmse:0.38008 val-rmse:0.41056 [371] train-rmse:0.38006 val-rmse:0.41056 [372] train-rmse:0.38000 val-rmse:0.41055 [373] train-rmse:0.37997 val-rmse:0.41055 [374] train-rmse:0.37991 val-rmse:0.41055 [375] train-rmse:0.37989 val-rmse:0.41054 [376] train-rmse:0.37986 val-rmse:0.41053 [377] train-rmse:0.37983 val-rmse:0.41052 [378] train-rmse:0.37980 val-rmse:0.41051 [379] train-rmse:0.37972 val-rmse:0.41052 [380] train-rmse:0.37970 val-rmse:0.41052 [381] train-rmse:0.37967 val-rmse:0.41051 [382] train-rmse:0.37964 val-rmse:0.41051 [383] train-rmse:0.37953 val-rmse:0.41047 [384] train-rmse:0.37949 val-rmse:0.41046 [385] train-rmse:0.37944 val-rmse:0.41045 [386] train-rmse:0.37939 val-rmse:0.41042 [387] train-rmse:0.37936 val-rmse:0.41041 [388] train-rmse:0.37931 val-rmse:0.41040 [389] train-rmse:0.37927 val-rmse:0.41040 [390] train-rmse:0.37911 val-rmse:0.41037 [391] train-rmse:0.37907 val-rmse:0.41035 [392] train-rmse:0.37901 val-rmse:0.41037 [393] train-rmse:0.37899 val-rmse:0.41036 [394] train-rmse:0.37893 val-rmse:0.41035 [395] train-rmse:0.37889 val-rmse:0.41034 [396] train-rmse:0.37879 val-rmse:0.41030 [397] train-rmse:0.37873 val-rmse:0.41029 [398] train-rmse:0.37868 val-rmse:0.41029 [399] train-rmse:0.37865 val-rmse:0.41029 [400] train-rmse:0.37861 val-rmse:0.41028 [401] train-rmse:0.37855 val-rmse:0.41029 [402] train-rmse:0.37850 val-rmse:0.41028 [403] train-rmse:0.37844 val-rmse:0.41027 [404] train-rmse:0.37841 val-rmse:0.41026 [405] train-rmse:0.37832 val-rmse:0.41021 [406] train-rmse:0.37825 val-rmse:0.41020 [407] train-rmse:0.37818 val-rmse:0.41018 [408] train-rmse:0.37812 val-rmse:0.41016 [409] train-rmse:0.37806 val-rmse:0.41017 [410] train-rmse:0.37797 val-rmse:0.41014 [411] train-rmse:0.37794 val-rmse:0.41013 [412] train-rmse:0.37792 val-rmse:0.41014 [413] train-rmse:0.37786 val-rmse:0.41012 [414] train-rmse:0.37782 val-rmse:0.41013 [415] train-rmse:0.37767 val-rmse:0.41010 [416] train-rmse:0.37765 val-rmse:0.41010 [417] train-rmse:0.37758 val-rmse:0.41009 [418] train-rmse:0.37752 val-rmse:0.41010 [419] train-rmse:0.37745 val-rmse:0.41010 [420] train-rmse:0.37744 val-rmse:0.41011 [421] train-rmse:0.37742 val-rmse:0.41011 [422] train-rmse:0.37738 val-rmse:0.41011 [423] train-rmse:0.37734 val-rmse:0.41011 [424] train-rmse:0.37729 val-rmse:0.41009 [425] train-rmse:0.37725 val-rmse:0.41008 [426] train-rmse:0.37721 val-rmse:0.41008 [427] train-rmse:0.37719 val-rmse:0.41007 [428] train-rmse:0.37715 val-rmse:0.41007 [429] train-rmse:0.37712 val-rmse:0.41008 [430] train-rmse:0.37702 val-rmse:0.41008 [431] train-rmse:0.37697 val-rmse:0.41007 [432] train-rmse:0.37694 val-rmse:0.41007 [433] train-rmse:0.37689 val-rmse:0.41008 [434] train-rmse:0.37685 val-rmse:0.41008 [435] train-rmse:0.37684 val-rmse:0.41009 [436] train-rmse:0.37677 val-rmse:0.41008 [437] train-rmse:0.37676 val-rmse:0.41007 [438] train-rmse:0.37674 val-rmse:0.41007 [439] train-rmse:0.37670 val-rmse:0.41007 [440] train-rmse:0.37664 val-rmse:0.41006 [441] train-rmse:0.37656 val-rmse:0.41006 [442] train-rmse:0.37652 val-rmse:0.41006 [443] train-rmse:0.37643 val-rmse:0.41003 [444] train-rmse:0.37638 val-rmse:0.41002 [445] train-rmse:0.37625 val-rmse:0.41000 [446] train-rmse:0.37617 val-rmse:0.40998 [447] train-rmse:0.37606 val-rmse:0.40995 [448] train-rmse:0.37602 val-rmse:0.40995 [449] train-rmse:0.37599 val-rmse:0.40995 [450] train-rmse:0.37596 val-rmse:0.40994 [451] train-rmse:0.37587 val-rmse:0.40994 [452] train-rmse:0.37578 val-rmse:0.40996 [453] train-rmse:0.37571 val-rmse:0.40992 [454] train-rmse:0.37569 val-rmse:0.40991 [455] train-rmse:0.37564 val-rmse:0.40990 [456] train-rmse:0.37562 val-rmse:0.40990 [457] train-rmse:0.37552 val-rmse:0.40990 [458] train-rmse:0.37547 val-rmse:0.40989 [459] train-rmse:0.37543 val-rmse:0.40988 [460] train-rmse:0.37540 val-rmse:0.40988 [461] train-rmse:0.37533 val-rmse:0.40986 [462] train-rmse:0.37531 val-rmse:0.40986 [463] train-rmse:0.37529 val-rmse:0.40986 [464] train-rmse:0.37527 val-rmse:0.40986 [465] train-rmse:0.37526 val-rmse:0.40986 [466] train-rmse:0.37525 val-rmse:0.40986 [467] train-rmse:0.37520 val-rmse:0.40983 [468] train-rmse:0.37518 val-rmse:0.40982 [469] train-rmse:0.37511 val-rmse:0.40980 [470] train-rmse:0.37509 val-rmse:0.40981 [471] train-rmse:0.37504 val-rmse:0.40981 [472] train-rmse:0.37502 val-rmse:0.40980 [473] train-rmse:0.37498 val-rmse:0.40979 [474] train-rmse:0.37494 val-rmse:0.40978 [475] train-rmse:0.37487 val-rmse:0.40978 [476] train-rmse:0.37483 val-rmse:0.40979 [477] train-rmse:0.37478 val-rmse:0.40978 [478] train-rmse:0.37475 val-rmse:0.40978 [479] train-rmse:0.37472 val-rmse:0.40979 [480] train-rmse:0.37469 val-rmse:0.40978 [481] train-rmse:0.37467 val-rmse:0.40978 [482] train-rmse:0.37462 val-rmse:0.40979 [483] train-rmse:0.37460 val-rmse:0.40978 [484] train-rmse:0.37458 val-rmse:0.40979 [485] train-rmse:0.37450 val-rmse:0.40979 [486] train-rmse:0.37448 val-rmse:0.40980 [487] train-rmse:0.37445 val-rmse:0.40978 [488] train-rmse:0.37442 val-rmse:0.40978 . yhat, interval = [y2_scaler.inv(y) * LATEST_INDEX for y in reg.predict(val_X[:,1:])] Result(&quot;XGBoost(ADJ)&quot;, yhat, interval, xgb_.get_score(importance_type=&#39;gain&#39;).values(), inv_y2, X.columns[1:]); . MAE (54,270) is 24.48% of mean RMSE (87,746) is 39.58% of mean MAPE 18.80% R2 Score is 0.832 . Multilayer Perceptron . Implementation . I will be implementing the multilayer perceptron through a modular layer-based implementation. . Layers . Each layer type will be implemented with a forward and backward methods. The forward method will feed input through the network, applying the current layer&#39;s function on the previous&#39; output. And the backward method will propagate the loss back through the network, updating trainable parameters along the way. I will only be implementing 2 layer types, the perceptron layer, with trainable weights, and the leaky ReLU non-linear activation function. . I will use Xavier or He initialisation for the weights: $$ text{Xavier}: W sim N left(0, frac 2{n_{in} + n_{out}} right) text{He}: W sim N left(0, frac 2{n_{in}} right) $$ Where, $N( mu, sigma^2)$: normal distribution with mean $ mu$ and variance $ sigma^2$ Since I will be using the leaky ReLU activation, I will likely use He initialisation. . The leaky ReLU activation function can be expressed as: $$ sigma(x) = begin{cases} x&amp; quad x ge 0 cx&amp; quad x &lt; 0 end{cases} sigma&#39;(x) = begin{cases} 1&amp; quad x &gt; 0 c&amp; quad x &lt; 0 DNE&amp; quad x = 0 end{cases} $$ Where, $c$: constant parameter, typically $0 &lt; c &lt; 1$ While the leaky ReLU function is non-differentiable at $x = 0$, it is relatively rare in the context of deep learning and can thus be set arbitrarily, typically to c or 1. . # algorithms/mlp/base.py NDArrayFunction = Callable[[ndarray], ndarray] LossFunction = Callable[[ndarray, ndarray], ndarray] WeightsInitFunction = Callable[[int, int], ndarray] class BaseOptimiser(object): def __init__(self): # self.reset() pass def reset(self): raise NotImplementedError() def update(self, grad:ndarray, id:str) -&gt; ndarray: raise NotImplementedError() class BaseLayer(object): def __init__(self, id:str=None): self.id = id # self.reset() def reset(self): self.inp = None def forward(self, inp:ndarray) -&gt; ndarray: raise NotImplementedError() def backward(self, grad:ndarray, optimiser:BaseOptimiser) -&gt; ndarray: raise NotImplementedError() class ActivationLayer(BaseLayer): def __init__(self, fn:NDArrayFunction, prime:NDArrayFunction, id:str=None): super().__init__(id) self.fn = fn self.prime = prime def forward(self, inp): self.inp = inp # storing x for back propagation return self.fn(inp) def backward(self, grad, optimiser): # optimiser parameter for consistency, will not be used return self.prime(self.inp) * grad # hadamard product - applying chain rule . # algorithms/mlp/layers.py class PerceptronLayer(BaseLayer): @staticmethod def xavier_init(n_in, n_out, seed=None): np.random.seed(seed) return np.random.normal( loc = 0.0, scale = np.sqrt(2/(n_in + n_out)), size = (n_in, n_out), ) @staticmethod def he_init(n_in, n_out, seed=None): np.random.seed(seed) return np.random.normal( loc = 0.0, scale = np.sqrt(2/n_in), size = (n_in, n_out), ) def __init__(self, n_in:int, n_out:int, weights_init:Union[str,WeightsInitFunction]=&quot;he&quot;, add_bias=True, id:str=None, seed=None): super().__init__(id) self.seed = seed self.n_in = n_in self.n_out = n_out self.add_bias = add_bias options = { &quot;he&quot;:self.he_init, &quot;kaiming&quot;:self.he_init, &quot;xavier&quot;:self.xavier_init, &quot;glorot&quot;:self.xavier_init, } self.weights_init = options.get(weights_init,weights_init) if not callable(self.weights_init): raise ValueError(f&quot;`weights_init` must be a string in {set(options.keys())} or callable(n_in, n_out, seed):ndarray[n_in, n_out]&quot;) self.reset() def reset(self): super().reset() # initialise weights self.weights = self.weights_init(self.n_in, self.n_out, self.seed) self.bias = np.zeros((1,self.n_out)) def forward(self, inp): self.inp = inp # storing input for back propagation return np.dot(self.inp, self.weights) + self.bias # y = w.x + b def backward(self, grad, optimiser): grad_inp = np.dot(grad, self.weights.T) # gradient of loss wrt input for backpropagation grad_weights = np.dot(self.inp.T, grad) # gradient of loss wrt weights grad_bias = np.mean(grad, axis=0) # gradient of loss wrt bias self.weights -= optimiser.update(grad_weights, self.id+&quot;weights&quot;) if self.add_bias: self.bias -= optimiser.update(grad_bias, self.id+&quot;bias&quot;) return grad_inp class LeakyReLU(ActivationLayer): def __init__(self, alpha:float, id:str=None): self.alpha = alpha super().__init__(self.fn, self.prime, id) def fn(self, inp): # x &gt;= 0: x # x &lt; 0: x * alpha return np.maximum(inp, self.alpha*inp) def prime(self, inp): # x &gt;= 0: 1 # x &lt; 0: alpha return np.where(inp &lt; 0, self.alpha, np.ones_like(inp)) . Parameter optimisation . I will be optimising trainable parameters using Adam (Adaptive Moment Estimation) optimisation which utilises momentum to make the gradient descent algorithm converge towards the minima faster. . $$ m := beta_1 m + (1 - beta_1)g v := beta_2 v + (1 - beta_2)g^2 $$$m$ and $v$ are the first (mean) and second (uncentered variance) moment estimates of the gradient $g$. As $m$ and $v$ are initialised as 0, they are biased towards zero, especially during the initial time steps and when the decay rates are small. Therefore, to counteract these biases, we need to compute bias-corrected first and second moment estimates: $$ hat m = frac{m}{1-( beta_1)^t} quad quad hat v = frac{v}{1-( beta_2)^t} $$ Finally we can update parameter $ phi$: $$ phi := phi - frac{ eta}{ sqrt{ hat v_t}+ epsilon} hat m_t $$ Where, $ phi$: Objective parameter $g$: Gradient of some cost function w.r.t to the objective parameter $m$: first moment (mean) estimate - initialised at 0 $v$: second moment (variance) estimate - initialised at 0 $t$: time step - intialised at 1, incremented after each iteration $ eta$: initial learning rate $ beta_1$: first moment estimate coefficient - default 0.9 $ beta_2$: second moment estimate coefficient - default 0.999 $ epsilon$: term to avoid zero division - default $10^{-8}$ . # algorithms/mlp/optimisers.py class AdamOptimiser(BaseOptimiser): def __init__(self, lr:float=0.01, beta1:float=0.9, beta2:float=0.999, eps:float=1e-8): self.lr = lr self.beta1 = beta1 self.beta2 = beta2 self.eps = eps self.reset() def reset(self): self.cache_m = dict() self.cache_v = dict() self.cache_t = dict() def update(self, grad, id): # retrieve previous iteration variables m = self.cache_m.get(id, 0) v = self.cache_v.get(id, 0) t = self.cache_t.get(id, 1) # update variables for current iteration self.cache_m[id] = self.beta1 * m + (1 - self.beta1) * grad self.cache_v[id] = self.beta2 * v + (1 - self.beta2) * grad ** 2 # bias-corrected variable m_corrected = self.cache_m[id] / (1 - self.beta1 ** t) v_corrected = self.cache_v[id] / (1 - self.beta2 ** t) self.cache_t[id] = t + 1 # delta calculation delta = self.lr * m_corrected / (np.sqrt(v_corrected) + self.eps) return delta . Neural network . # algorithms/mlp/model.py from sklearn.base import BaseEstimator def mse(y, yhat): return np.mean(np.power(y-yhat, 2)) def mse_prime(y, yhat): return 2*(yhat-y)/y.size class NeuralNetwork(BaseEstimator): def __init__(self, layers:list[BaseLayer], optimiser:BaseOptimiser, loss_fn:LossFunction=mse, loss_prime:LossFunction=mse_prime): self.layers = layers self.loss_fn = loss_fn self.loss_prime = loss_prime self.optimiser = optimiser def reset(self): # resetting model components self.history = dict() self.optimiser.reset() for l in self.layers: l.reset() def predict(self,X:ndarray) -&gt; ndarray: out = np.asarray(X) # forward propagation for l in self.layers: out = l.forward(out) # clearing cached layer inputs for layer in self.layers: layer.inp = None return out def backpropagate(self, X:ndarray, y:ndarray) -&gt; float: out = self.predict(X) grad_out = self.loss_prime(y,out) for l in self.layers[::-1]: # propagate gradient backwards through network grad_out = l.backward(grad_out,self.optimiser) return self.loss_fn(y,out) @staticmethod def batches(X:ndarray, y:ndarray, batch_size:int, shuffle:bool) -&gt; tuple[ndarray, ndarray]: n = len(X) # random indices if shuffling enabled idx = np.random.permutation(n) if shuffle else np.arange(n) for i in range(0,n,batch_size): batch = idx[i:i+batch_size] # taking indices from index list yield X[batch], y[batch] # lazy evaluation def fit(self, X:ndarray, y:ndarray, val_X:ndarray = None, val_y:ndarray = None, epochs:int = 1000, batch_size:int = 1, early_stopping:int = 20, delta:float = 0.01, shuffle:bool = False, verbose:bool = True ): for i, l in enumerate(self.layers): if l.id is None: l.id = f&quot;layer{i}&quot; # X,y argument validation X, y = np.asarray(X), np.asarray(y) if X.ndim == 1 or y.ndim == 1: raise ValueError(&quot;`X` and `y` arrays must be 2 dimensional&quot;) self.history = {&quot;loss&quot;:[]} # initialising training loss history # val_X,val_y argument validation if val_X is not None and val_y is not None: val_X, val_y = np.asarray(val_X), np.asarray(val_y) if val_X.ndim == 1 or val_y.ndim == 1: raise ValueError(&quot;`val_X` and `val_y` arrays must be 2 dimensional&quot;) self.history[&quot;val_loss&quot;] = [] # initialising validation loss history patience = None # early_stopping argument validation if isinstance(early_stopping, int) and early_stopping &gt; 0: patience = early_stopping best_loss = np.inf # intialising best loss for model improvement calculation else: warnings.warn(f&quot;`early_stopping` must be positive non-zero integer not `{early_stopping}`, defaulting to None&quot;) for i in (pbar:=tqdm(range(epochs), disable=not verbose)): # end training if not improved if patience is not None and patience &lt;= 0: pbar.close() if verbose: print(f&quot;Early stopping at epoch {i}&quot;) break epoch_loss = 0 # split training set into batches for less intensive computation for Xb, yb in self.batches(X,y,batch_size,shuffle): epoch_loss += self.backpropagate(Xb,yb) * len(Xb) # back propagating model and summing error for batches epoch_loss /= len(X) # taking mean error across all batches self.history[&quot;loss&quot;].append(epoch_loss) # testing model on validation set if provided if &quot;val_loss&quot; in self.history: self.history[&quot;val_loss&quot;].append(self.loss_fn(self.predict(val_X), val_y)) # logging epoch info if enabled if verbose: msg = {&quot;loss&quot;: self.history[&quot;loss&quot;][-1]} if &quot;val_loss&quot; in self.history: msg[&quot;val_loss&quot;] = self.history[&quot;val_loss&quot;][-1] if patience is not None: msg[&quot;patience&quot;] = patience pbar.set_postfix(msg) # skip to next iteration if early stopping is disabled if patience is None: continue # if model has improved greater than minimum delta if best_loss - epoch_loss &gt;= delta: patience = early_stopping # reset patience best_loss = epoch_loss continue # skip to next iteration patience -= 1 # decrement patience (code wont be reached if model improved) # clearing cached layer inputs for layer in self.layers: layer.inp = None return self . Hyperparameter Optimisation . from algorithms.mlp.layers import PerceptronLayer, LeakyReLU from algorithms.mlp.optimisers import AdamOptimiser from algorithms.mlp.model import NeuralNetwork def build_mlp(hidden_layers, n_units, lrelu_alpha): layers = [ PerceptronLayer(len(X.columns), n_units, seed=SEED), LeakyReLU(lrelu_alpha) ] for i in range(hidden_layers-1): layers += [ PerceptronLayer(n_units, n_units, seed=SEED), LeakyReLU(lrelu_alpha) ] layers.append(PerceptronLayer(n_units, 1, add_bias=False)) return NeuralNetwork(layers, AdamOptimiser()) . pbounds = { &quot;hidden_layers&quot;: hp.uniformint(&quot;hidden_layers&quot;, 1, 4), &quot;n_units&quot;: hp.uniformint(&quot;n_units&quot;, 24, 56), &quot;lrelu_alpha&quot;: hp.uniform(&quot;lrelu_alpha&quot;, 0, 0.5), } n = 100_000 idx = np.random.permutation(len(tr_X))[:n] sample_x, sample_y = tr_X[idx].astype(np.float32), tr_y[idx].astype(np.float32) . fn = &quot;models/_mlp_trials.hyperopt&quot; trials = Trials() if not os.path.exists(fn) else load(fn) with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;) result = fmin( fn = objective( model = build_mlp, X = sample_x, y = sample_y.reshape(-1,1), cv = 5, fit_params = { &quot;early_stopping&quot;: 5, # speed up training &quot;batch_size&quot;: 1024, &quot;verbose&quot;: False, } ), space = pbounds, algo = tpe.suggest, max_evals = 100, trials = trials ) if not os.path.exists(fn): dump(trials, fn) nnparams = {k:v[0] for k,v in trials.best_trial[&quot;misc&quot;][&quot;vals&quot;].items()} nnparams[&quot;hidden_layers&quot;] = int(round(nnparams[&quot;hidden_layers&quot;])) nnparams[&quot;n_units&quot;] = int(round(nnparams[&quot;n_units&quot;])) . 100%|ââââââââââ| 100/100 [10:48&lt;00:00, 6.49s/trial, best loss: 0.25280905636164314] . f, axs = plt.subplots(2, (len(trials.vals)+1)//2, sharey=True, figsize=(10,5)) axs[0,0].set_ylabel(&quot;rmse&quot;) axs[1,0].set_ylabel(&quot;rmse&quot;) for k, ax in zip(trials.vals, axs.flatten()): ax.scatter(trials.vals[k], trials.losses(), 5) ax.set_xlabel(k) print(f&quot;&quot;&quot;best loss: {trials.best_trial[&quot;result&quot;][&quot;loss&quot;]}, params: {pformat(nnparams)}&quot;&quot;&quot;) f.tight_layout() . best loss: 0.25280905636164314, params: {&#39;hidden_layers&#39;: 2, &#39;lrelu_alpha&#39;: 0.04031941259789426, &#39;n_units&#39;: 35} . Fitting and evaluating . fn = &quot;models/MultilayerPerceptron.bin&quot; if not os.path.exists(fn): nnparams[&quot;hidden_layers&quot;] = 3 # 3-5 layers were around the same reg = build_mlp(**nnparams).fit( tr_X, tr_y.reshape(-1,1), val_X = val_X, val_y = val_y.reshape(-1,1), epochs = 1000, batch_size = 1024, early_stopping = 25, delta=0.005, ) dump(reg, fn) reg = ConformalRegression(load(fn)).calibrate(cal_X, cal_y.reshape(-1,1)) . 7%|â | 72/1000 [13:15&lt;2:50:51, 11.05s/it, loss=0.161, val_loss=0.161, patience=1] . Early stopping at epoch 72 . f, ax = plt.subplots() ax.plot(reg.model.history[&quot;loss&quot;], &quot;--&quot;, label=&quot;Training loss&quot;) ax.plot(reg.model.history[&quot;val_loss&quot;], &quot;--&quot;, label=&quot;Validation loss&quot;) ax.set_title(&quot;Fitting multilayer perceptron model&quot;) ax.set_xlabel(&quot;epoch&quot;) ax.set_ylabel(&quot;loss (mse)&quot;) ax.legend(); . feat_imp = permutation_importance(lambda x: reg.predict(x, alpha=None), val_X[:100_000], val_y[:100_000].reshape(-1,1)) . yhat, interval = [y_scaler.inv(y) for y in reg.predict(val_X)] yhat = yhat.reshape(-1) Result(&quot;MultilayerPerceptron&quot;, yhat, interval, feat_imp); . MAE (41,696) is 18.81% of mean RMSE (65,708) is 29.64% of mean MAPE 23.26% R2 Score is 0.838 . Conclusion . from IPython.display import display plot = pd.DataFrame(results).T[[&quot;r2&quot;,&quot;mape&quot;]] display(plot) f,axs = plt.subplots(2) asc = { &quot;mape&quot;:False, &quot;r2&quot;:True, } plot = plot.rename(index=lambda x: x if len(x) &lt;= 10 else x[:7]+&quot;...&quot;) for col,ax in zip(plot, axs.flatten()): ax.set_title(col) plot[col].sort_values(ascending=asc[col]).plot(kind=&quot;barh&quot;, ax=ax) f.tight_layout() . r2 mape . LinearRegression 0.479313 | 47.352439 | . KNearestNeighbours(ADJ) 0.767500 | 21.275510 | . DecisionTree(ADJ) 0.657260 | 28.470305 | . RandomForest(ADJ) 0.822758 | 19.384993 | . XGBoost(ADJ) 0.832487 | 18.801916 | . MultilayerPerceptron 0.838236 | 23.260539 | . SOURCE = &quot;models/MultilayerPerceptron.bin&quot; DESTINATION = os.path.join(SERVER_DIR, &quot;model.bin&quot;) os.system(f&quot;cp {SOURCE} {DESTINATION}&quot;); . Since I want to use conformal prediction, I will also pre-calibrate and export the calibration scores to the server directory. By calibrating beforehand, I will not need to calibrate the model in the server, avoiding any intensive computation on the Heroku server-side, which could use resources not available to free users. . calibrated = ConformalRegression(load(SOURCE)).calibrate(cal_X, cal_y.reshape(-1,1)) dump(calibrated.resid_, os.path.join(SERVER_DIR, &quot;conf_resid.bin&quot;)); .",
            "url": "https://uzair223.github.io/posts/house-prices/2022/05/08/p4-model-selection.html",
            "relUrl": "/house-prices/2022/05/08/p4-model-selection.html",
            "date": " â¢ May 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "ML Instant House Valuation: Part 3 - Data exploration and transformation",
            "content": "import os, json import numpy as np import pandas as pd import dask.dataframe as dd # visualisation from pprint import pprint, pformat import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline plt.rcParams[&quot;figure.figsize&quot;] = (7, 5) plt.rcParams[&quot;figure.dpi&quot;] = 80 sns.set_theme(style=&quot;whitegrid&quot;) . . Let&#39;s begin by outputting some of the first few lines of the dataset. The main aim of this section is to completely transform this into purely numerical values. . PRICE POSTCODE OLD_NEW PROPERTY_TYPE BUILT_FORM TOTAL_FLOOR_AREA GLAZED_TYPE EXTENSION_COUNT NUMBER_HABITABLE_ROOMS CONSTRUCTION_AGE_BAND TENURE . DATE_OF_TRANSFER . 1995-01-01 16000.0 | HX1 4NG | N | House | Enclosed Mid-Terrace | 99.00 | double glazing, unknown install date | 0.0 | 4.0 | England and Wales: before 1900 | owner-occupied | . 1995-01-02 35000.0 | NN1 4LL | N | House | Mid-Terrace | 85.00 | double glazing installed before 2002 | 1.0 | 4.0 | England and Wales: 1900-1929 | owner-occupied | . 1995-01-03 15000.0 | NE34 8TF | N | House | Semi-Detached | 74.00 | double glazing installed before 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | . 1995-01-03 48000.0 | HR8 2DB | N | Bungalow | Detached | 78.00 | double glazing installed before 2002 | 1.0 | 3.0 | England and Wales: 1950-1966 | owner-occupied | . 1995-01-03 82000.0 | TR13 8BP | N | Bungalow | Detached | 118.88 | double glazing installed during or after 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | . However, a table of the data points is not very helpful in analysis, therefore I will do some visualisation first. . Dataset as a time series . I will begin by visualising the dataset with the median monthly housing price as plotted through time. . As shown, the average price of a property has grown, almost exponentially, throughout the years. This behaviour could be leveraged by a prediction model, however for ideal predictions, this needs to be stationarised as to be able to make predictions regardless of the time the training data was observed. . Detrending dataset . In order to be able to predict the price of a property regardless of time, I need to detrend/stationarise the prices using the house price index. I will use the adjusted prices for the sample outputs in non-extrapolation models (such as decision tree ensembles), as well as visualisation and outlier removal. Additionally, I will add the HPI and data as features and will use the raw prices as outputs for extrapolation models that can learn the time correlation instead. . To adjust the prices: $$ textit{Adjusted price} = frac{ textit{Price in y}}{ textit{y index}} $$ Where, $y$ : Date of observation . To bring the adjusted price back to the current date, or any date for that matter, we simply need to multiply by the index of the target date: $$ textit{Price in x} = textit{x index} times textit{Adjusted price} = textit{x index} times frac{ textit{Price in y}}{ textit{y index}} $$ Where, $x$ : Target date $y$ : Date of observation . hpi = pd.read_csv(HPI_PATH, parse_dates=[&quot;Date&quot;]).set_index(&quot;Date&quot;).Index hpi.plot().set_title(&quot;House Price Index&quot;); . idx = pd.DatetimeIndex(df.index.values.astype(&quot;datetime64[M]&quot;)) # converting date of transfers to start of month dates # filling any missing dates in house price index missing = idx.unique()[~idx.unique().isin(hpi.index)] if len(missing) &gt; 0: hpi = pd.concat([hpi, pd.Series(index=missing, dtype=&quot;float64&quot;)]).fillna(method=&quot;bfill&quot;) # backfill nans with previous index values hpi_reindexed = hpi[idx].values # map df dates to multipliers df.insert(0, &quot;PRICE_ADJ&quot;, df.PRICE.values / hpi_reindexed) # adjusting prices: price * multiplier . df[&quot;MONTH&quot;] = df.index.month df[&quot;YEAR&quot;] = df.index.year df[&quot;HPI&quot;] = hpi[idx].values . I will now visualise the inflation-adjusted time series, as compared to the original. . The effects of the 2008 financial crisis are very apparent, and we can see more recent instability in house prices due to the COVID-19 pandemic - as it drastically breaks from the seasonal pattern in 2020. Additionally, there is a visible seasonality in the series, as we see a rise in price towards summer months, and a fall in the winter. . Feature engineering . Now, I will transform the dataset so that it is more interpretable by a machine learning algorithm; esentially, I need to convert the dataset to numeric values instead of strings. I will also reduce some of the dimensionality currently present in the data; including the cardinality of the categorical columns, and the number of features in the dataset. . Geospatial information . Currently, the postcode of a property is completely uninterpretable by a predictive model. In order to still leverage this useful location data, I will transform my dataset by using external coordinate data instead. . pcdLookup = ( pd.read_csv(PCD_PATH) # data/raw/postcode-lookup.csv .set_index(&quot;PCDS&quot;) ) pcdLookup.head() . LAT LNG . PCDS . AB1 0AA 57.101474 | -2.242851 | . AB1 0AB 57.102554 | -2.246308 | . AB1 0AD 57.100556 | -2.248342 | . AB1 0AE 57.084444 | -2.255708 | . AB1 0AF 57.096656 | -2.258102 | . df = pd.merge(df, pcdLookup, how=&quot;left&quot;, left_on=&quot;POSTCODE&quot;, right_index=True).dropna() # mapping postcodes to lat, long, imd df = df.drop(columns=[&quot;POSTCODE&quot;]) df.head() . PRICE_ADJ PRICE OLD_NEW PROPERTY_TYPE BUILT_FORM TOTAL_FLOOR_AREA GLAZED_TYPE EXTENSION_COUNT NUMBER_HABITABLE_ROOMS CONSTRUCTION_AGE_BAND TENURE MONTH YEAR HPI LAT LNG . DATE_OF_TRANSFER . 1995-01-01 605.142016 | 16000.0 | N | House | Enclosed Mid-Terrace | 99.00 | double glazing, unknown install date | 0.0 | 4.0 | England and Wales: before 1900 | owner-occupied | 1 | 1995 | 26.440075 | 53.723903 | -1.882344 | . 1995-01-02 1323.748160 | 35000.0 | N | House | Mid-Terrace | 85.00 | double glazing installed before 2002 | 1.0 | 4.0 | England and Wales: 1900-1929 | owner-occupied | 1 | 1995 | 26.440075 | 52.245071 | -0.878169 | . 1995-01-03 567.320640 | 15000.0 | N | House | Semi-Detached | 74.00 | double glazing installed before 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | 1 | 1995 | 26.440075 | 54.961605 | -1.419408 | . 1995-01-03 1815.426048 | 48000.0 | N | Bungalow | Detached | 78.00 | double glazing installed before 2002 | 1.0 | 3.0 | England and Wales: 1950-1966 | owner-occupied | 1 | 1995 | 26.440075 | 52.034265 | -2.432491 | . 1995-01-03 3101.352831 | 82000.0 | N | Bungalow | Detached | 118.88 | double glazing installed during or after 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | 1 | 1995 | 26.440075 | 50.092749 | -5.265793 | . To show the significance of the coordinate data in relation to this dataset, I will produce a scatter plot of the coordinates with a colour map of the price. . The plot shows a very clear relationship between location and price; further south, towards London, the price of a property is largely significantly higher than in the north. This behaviour will be very useful for a model to learn, and will probably be the most helpful features in the dataset. . Categorical data . Categorical data can be very useful to a predictive model, if the categories are formatted properly. Since all the data passed into the algorithms must be numerical, the categorical columns must be encoded to integer/float values. One way of doing this is One-hot Encoding where each category in a column is binary mapped to a zero or one. However, one-hot encoding each category will lead to an extremely high dimensionality, since each individual categorical column will be mapped to several new columns. Therefore, I will be using Ordinal Encoding, which will number/rank the categories instead. Before doing this, I will reduce the high cardinality in the data by removing categories in columns with low counts, and merging categories where appropriate. . for col in df.select_dtypes(&quot;category&quot;): df[col] = df[col].cat.remove_unused_categories() . Tidying categories and reducing cardinality . I will begin by tidying up the PROPERTY_TYPE and BUILT_FORM columns. . print(df.PROPERTY_TYPE.value_counts()) df = df[df.PROPERTY_TYPE !=&quot;Park home&quot;] # Park home category only has 19 records df[&quot;PROPERTY_TYPE&quot;] = df.PROPERTY_TYPE.cat.remove_unused_categories() print(&quot; nPROPERTY_TYPE - New categories:&quot;, list(df.PROPERTY_TYPE.cat.categories)) . House 4091970 Flat 882359 Bungalow 477227 Maisonette 115324 Park home 19 Name: PROPERTY_TYPE, dtype: int64 PROPERTY_TYPE - New categories: [&#39;Bungalow&#39;, &#39;Flat&#39;, &#39;House&#39;, &#39;Maisonette&#39;] . print(df.BUILT_FORM.value_counts()) df[&quot;BUILT_FORM&quot;] = ( df.BUILT_FORM .str.replace(&quot;Enclosed &quot;, &quot;&quot;) .str.replace(&quot;Mid-&quot;,&quot;&quot;) .str.replace(&quot;End-&quot;,&quot;&quot;) .astype(&quot;category&quot;) .cat.remove_unused_categories() ) print(&quot; nBUILT_FORM - New categories:&quot;, list(df.BUILT_FORM.cat.categories)) . Semi-Detached 1800563 Mid-Terrace 1666569 Detached 1241212 End-Terrace 720056 Enclosed End-Terrace 78718 Enclosed Mid-Terrace 59762 Name: BUILT_FORM, dtype: int64 BUILT_FORM - New categories: [&#39;Detached&#39;, &#39;Semi-Detached&#39;, &#39;Terrace&#39;] . I will now use the columns to make a new property type column which is perhaps more useful. Ultimately, the &quot;usefulness&quot; of this column will be determined by feature selection but it could be better than the other two on their own. The &quot;Flat&quot; and &quot;Maisonette&quot; categories will not be coupled with their accompanying built form category types as, logically, it doesn&#39;t make much sense. . prp_map = { (&quot;House&quot;,&quot;Detached&quot;): &quot;Detached house&quot;, (&quot;House&quot;,&quot;Semi-Detached&quot;): &quot;Semi-detached house&quot;, (&quot;House&quot;,&quot;Terrace&quot;): &quot;Terrace house&quot;, (&quot;Bungalow&quot;,&quot;Detached&quot;): &quot;Detached bungalow&quot;, (&quot;Bungalow&quot;,&quot;Semi-Detached&quot;): &quot;Semi-detached bungalow&quot;, (&quot;Bungalow&quot;,&quot;Terrace&quot;): &quot;Terrace bungalow&quot;, (&quot;Flat&quot;,&quot;Detached&quot;): &quot;Flat&quot;, (&quot;Flat&quot;,&quot;Semi-Detached&quot;): &quot;Flat&quot;, (&quot;Flat&quot;,&quot;Terrace&quot;): &quot;Flat&quot;, (&quot;Maisonette&quot;,&quot;Detached&quot;): &quot;Maisonette&quot;, (&quot;Maisonette&quot;,&quot;Semi-Detached&quot;): &quot;Maisonette&quot;, (&quot;Maisonette&quot;,&quot;Terrace&quot;): &quot;Maisonette&quot;, } df[&quot;N_PROPERTY_TYPE&quot;] = list(zip(df.PROPERTY_TYPE, df.BUILT_FORM)) df[&quot;N_PROPERTY_TYPE&quot;] = df.N_PROPERTY_TYPE.map(prp_map).astype(&quot;category&quot;) print(&quot;N_PROPERTY_TYPE - categories:&quot;, list(df.N_PROPERTY_TYPE.cat.categories)) . N_PROPERTY_TYPE - categories: [&#39;Detached bungalow&#39;, &#39;Detached house&#39;, &#39;Flat&#39;, &#39;Maisonette&#39;, &#39;Semi-detached bungalow&#39;, &#39;Semi-detached house&#39;, &#39;Terrace bungalow&#39;, &#39;Terrace house&#39;] . Now I will continue tidying up the remaining columns. . print(df.GLAZED_TYPE.value_counts()) df = df[~df.GLAZED_TYPE.isin([ &quot;triple glazing&quot;, &quot;double, known data&quot;, &quot;triple, known data&quot; # The categories don&#39;t have a lot of records and can therefore be removed ])] df[&quot;GLAZED_TYPE&quot;] = df.GLAZED_TYPE.cat.remove_unused_categories() print(&quot; nGLAZED_TYPE - New categories:&quot;, list(df.GLAZED_TYPE.cat.categories)) . double glazing installed before 2002 1992825 double glazing, unknown install date 1732087 double glazing installed during or after 2002 1719791 secondary glazing 66849 single glazing 45732 triple glazing 8175 double, known data 1321 triple, known data 100 Name: GLAZED_TYPE, dtype: int64 GLAZED_TYPE - New categories: [&#39;double glazing installed before 2002&#39;, &#39;double glazing installed during or after 2002&#39;, &#39;double glazing, unknown install date&#39;, &#39;secondary glazing&#39;, &#39;single glazing&#39;] . print(df.CONSTRUCTION_AGE_BAND.value_counts()) df[&quot;CONSTRUCTION_AGE_BAND&quot;] = ( df.CONSTRUCTION_AGE_BAND .str.replace(&quot;England and Wales: &quot;, &quot;&quot;) .replace([&quot;2007-2011&quot;, &quot;2012 onwards&quot;], &quot;2007 onwards&quot;) # Merging `2007-2011` and `2012 onwards` to `2007 onwards` columns .astype(&quot;category&quot;) ) print(&quot; nCONSTRUCTION_AGE_BAND - New categories:&quot;, list(df.CONSTRUCTION_AGE_BAND.cat.categories)) . England and Wales: 1900-1929 980478 England and Wales: 1930-1949 930925 England and Wales: 1950-1966 880279 England and Wales: 1967-1975 617905 England and Wales: before 1900 577752 England and Wales: 1983-1990 371020 England and Wales: 1996-2002 304103 England and Wales: 1976-1982 293451 England and Wales: 2003-2006 269729 England and Wales: 1991-1995 193808 England and Wales: 2007 onwards 122659 England and Wales: 2007-2011 14173 England and Wales: 2012 onwards 1002 Name: CONSTRUCTION_AGE_BAND, dtype: int64 CONSTRUCTION_AGE_BAND - New categories: [&#39;1900-1929&#39;, &#39;1930-1949&#39;, &#39;1950-1966&#39;, &#39;1967-1975&#39;, &#39;1976-1982&#39;, &#39;1983-1990&#39;, &#39;1991-1995&#39;, &#39;1996-2002&#39;, &#39;2003-2006&#39;, &#39;2007 onwards&#39;, &#39;before 1900&#39;] . print(df.TENURE.value_counts()) df[&quot;TENURE&quot;] = ( df.TENURE .replace(&quot;Owner-occupied&quot;, &quot;owner-occupied&quot;) # merging categories .replace(&quot;Rented (private)&quot;, &quot;rental (private)&quot;) .replace(&quot;Rented (social)&quot;, &quot;rental (social)&quot;) .replace(&quot;Not defined - use in the case of a new dwelling for which the intended tenure in not known. It is no&quot;,&quot;unknown&quot;) .cat.remove_unused_categories() ) print(&quot; nTENURE - New categories:&quot;, list(df.TENURE.cat.categories)) . owner-occupied 3988859 rental (private) 1140190 Owner-occupied 154158 unknown 108172 rental (social) 86042 Rented (private) 72821 Rented (social) 4925 Not defined - use in the case of a new dwelling for which the intended tenure in not known. It is no 2117 Name: TENURE, dtype: int64 TENURE - New categories: [&#39;owner-occupied&#39;, &#39;rental (private)&#39;, &#39;rental (social)&#39;, &#39;unknown&#39;] . Category encoding . I will encode the categories within a column by ranking them in order based on their average property price. I will later store these mappings to a .json file to encode inputs when serving requests to the API. . def RankingEncode(df, target, categories): if isinstance(categories, str): categories = [categories] return { cat: { k: i for i, k in enumerate( df.groupby(cat)[target].median().sort_values().index.values ) } for cat in categories } . mappings = RankingEncode(df, &quot;PRICE_ADJ&quot;, df.select_dtypes(&quot;category&quot;)) . The categorical variables have been assigned increasing integers corresponding to their effect on the price, and the OLD_NEW column has essentially been binary encoded. However, the ranked order for the CONSTRUCTION_AGE_BAND column does not make logical sense; therefore I will rank the categories in chronological order instead. . mappings[&quot;CONSTRUCTION_AGE_BAND&quot;] = { k: i for i, k in enumerate( [&quot;before 1900&quot;, *list(df.CONSTRUCTION_AGE_BAND.cat.categories)[:-1]] ) } mappings[&quot;CONSTRUCTION_AGE_BAND&quot;] . {&#39;before 1900&#39;: 0, &#39;1900-1929&#39;: 1, &#39;1930-1949&#39;: 2, &#39;1950-1966&#39;: 3, &#39;1967-1975&#39;: 4, &#39;1976-1982&#39;: 5, &#39;1983-1990&#39;: 6, &#39;1991-1995&#39;: 7, &#39;1996-2002&#39;: 8, &#39;2003-2006&#39;: 9, &#39;2007 onwards&#39;: 10} . for col, map_ in mappings.items(): df[col] = df[col].map(map_) df.head() . PRICE_ADJ PRICE OLD_NEW PROPERTY_TYPE BUILT_FORM TOTAL_FLOOR_AREA GLAZED_TYPE EXTENSION_COUNT NUMBER_HABITABLE_ROOMS CONSTRUCTION_AGE_BAND TENURE MONTH YEAR HPI LAT LNG N_PROPERTY_TYPE . DATE_OF_TRANSFER . 1995-01-01 605.142016 | 16000.0 | 0 | 1 | 0 | 99.00 | 0 | 0.0 | 4.0 | 0 | 3 | 1 | 1995 | 26.440075 | 53.723903 | -1.882344 | 1 | . 1995-01-02 1323.748160 | 35000.0 | 0 | 1 | 0 | 85.00 | 1 | 1.0 | 4.0 | 1 | 3 | 1 | 1995 | 26.440075 | 52.245071 | -0.878169 | 1 | . 1995-01-03 567.320640 | 15000.0 | 0 | 1 | 1 | 74.00 | 1 | 0.0 | 4.0 | 4 | 1 | 1 | 1995 | 26.440075 | 54.961605 | -1.419408 | 4 | . 1995-01-03 1815.426048 | 48000.0 | 0 | 3 | 2 | 78.00 | 1 | 1.0 | 3.0 | 3 | 3 | 1 | 1995 | 26.440075 | 52.034265 | -2.432491 | 6 | . 1995-01-03 3101.352831 | 82000.0 | 0 | 3 | 2 | 118.88 | 2 | 0.0 | 4.0 | 4 | 1 | 1 | 1995 | 26.440075 | 50.092749 | -5.265793 | 6 | . Now that all the columns are encoded, we can more clearly see their importance as features to a property&#39;s price, and can filter them as such. . Feature selection . To reduce the number of unnecessary features, I will test the features&#39; importances, and remove features that are not significant. . I will use two metrics to evaluate feature importance: . Mutual information | Pearson&#39;s correlation coefficient, calculated as such:&ensp;$r = frac{(x - bar x)(y - bar y) }{ sigma x cdot sigma y}$ | 100,000 sample rows . Correlation: . PRICE OLD_NEW PROPE... BUILT... TOTAL... GLAZE... EXTEN... NUMBE... CONST... TENURE MONTH YEAR HPI LAT LNG N_PRO... . PRICE nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . OLD_NEW -0.033 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . PROPE... 0.027 | -0.129 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . BUILT... 0.123 | -0.023 | 0.302 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . TOTAL... 0.350 | -0.049 | 0.113 | 0.326 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . GLAZE... 0.042 | 0.092 | -0.070 | 0.023 | 0.035 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . EXTEN... 0.112 | -0.089 | 0.082 | 0.111 | 0.347 | -0.041 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . NUMBE... 0.257 | -0.080 | 0.173 | 0.343 | 0.738 | 0.004 | 0.368 | nan | nan | nan | nan | nan | nan | nan | nan | nan | . CONST... -0.031 | 0.305 | -0.100 | 0.171 | -0.095 | 0.150 | -0.278 | -0.128 | nan | nan | nan | nan | nan | nan | nan | nan | . TENURE 0.078 | -0.133 | 0.197 | 0.214 | 0.179 | -0.004 | 0.111 | 0.221 | -0.037 | nan | nan | nan | nan | nan | nan | nan | . MONTH 0.004 | 0.009 | 0.011 | 0.023 | 0.006 | 0.003 | 0.002 | 0.009 | 0.012 | 0.023 | nan | nan | nan | nan | nan | nan | . YEAR 0.162 | -0.204 | 0.077 | 0.064 | 0.029 | -0.021 | 0.048 | 0.036 | -0.005 | 0.256 | -0.032 | nan | nan | nan | nan | nan | . HPI 0.159 | -0.162 | 0.055 | 0.043 | 0.017 | -0.024 | 0.041 | 0.019 | 0.011 | 0.210 | 0.025 | 0.963 | nan | nan | nan | nan | . LAT -0.161 | 0.026 | 0.036 | -0.034 | -0.027 | -0.049 | -0.013 | 0.021 | -0.021 | 0.006 | -0.001 | -0.007 | 0.005 | nan | nan | nan | . LNG 0.131 | -0.013 | -0.048 | -0.019 | -0.003 | 0.016 | -0.010 | -0.046 | -0.005 | -0.039 | -0.006 | -0.021 | -0.025 | -0.223 | nan | nan | . N_PRO... 0.143 | -0.027 | 0.316 | 0.898 | 0.379 | 0.020 | 0.143 | 0.408 | 0.164 | 0.225 | 0.025 | 0.066 | 0.044 | -0.030 | -0.013 | nan | . I will now choose the 6 best features. Since YEAR and HPI are highly correlated, and both perform almost equally as well, I will somewhat arbitrarily choose the HPI column between them. . df = df[[&quot;PRICE&quot;, &quot;PRICE_ADJ&quot;, &quot;HPI&quot;, &quot;LAT&quot;, &quot;LNG&quot;, &quot;TOTAL_FLOOR_AREA&quot;, &quot;NUMBER_HABITABLE_ROOMS&quot;, &quot;N_PROPERTY_TYPE&quot;]] df[&quot;PROPERTY_TYPE&quot;] = df.pop(&quot;N_PROPERTY_TYPE&quot;) df . PRICE PRICE_ADJ HPI LAT LNG TOTAL_FLOOR_AREA NUMBER_HABITABLE_ROOMS PROPERTY_TYPE . DATE_OF_TRANSFER . 1995-01-01 16000.0 | 605.142016 | 26.440075 | 53.723903 | -1.882344 | 99.00 | 4.0 | 1 | . 1995-01-02 35000.0 | 1323.748160 | 26.440075 | 52.245071 | -0.878169 | 85.00 | 4.0 | 1 | . 1995-01-03 15000.0 | 567.320640 | 26.440075 | 54.961605 | -1.419408 | 74.00 | 4.0 | 4 | . 1995-01-03 48000.0 | 1815.426048 | 26.440075 | 52.034265 | -2.432491 | 78.00 | 3.0 | 6 | . 1995-01-03 82000.0 | 3101.352831 | 26.440075 | 50.092749 | -5.265793 | 118.88 | 4.0 | 6 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 2022-02-24 1850000.0 | 12657.155077 | 146.162387 | 51.568788 | -0.780082 | 238.00 | 8.0 | 1 | . 2022-02-24 170000.0 | 1163.089926 | 146.162387 | 51.842156 | -2.232514 | 76.97 | 4.0 | 4 | . 2022-02-24 575000.0 | 3933.980632 | 146.162387 | 51.038076 | -0.897733 | 121.00 | 6.0 | 7 | . 2022-02-25 297950.0 | 2038.486138 | 146.162387 | 50.776231 | 0.128243 | 93.00 | 4.0 | 4 | . 2014-12-12 248000.0 | 2473.662336 | 100.256206 | 50.867424 | -2.963388 | 104.00 | 6.0 | 7 | . 5557284 rows Ã 8 columns . Distribution and Skewness . Skewed data can be described as data where the distribution curve appears to be asymmetrical. Skewed data can either be left-skewed or right-skewed. Data with right skew (positive skew) will have a mean &gt; median &gt; mode. Data with left skew (negative skew) will have a mean &lt; median &lt; mode. . . Skewed data can adversely affect a model&#39;s performance especially in regression-based models, such as linear regression, where the model assumes the data follows normal distribution and has no outliers. Therefore, the model can tend to favour more extreme values, rather than values which are within the true range. Tree-based models, which I may use, are not susceptible to this sort of situation, however, in order to try other models as a possibility, I will need to transform the data to properly balance it. . skew = [&quot;PRICE_ADJ&quot;, &quot;TOTAL_FLOOR_AREA&quot;, &quot;NUMBER_HABITABLE_ROOMS&quot;] df[skew].skew() . PRICE_ADJ 264.080204 TOTAL_FLOOR_AREA 270.696094 NUMBER_HABITABLE_ROOMS 2.919180 dtype: float64 . The graph clearly shows extreme positive-skew in the price, floor area, room, and crime columns. Since the dataset should ideally follow normal distribution, I will &quot;unskew&quot; this data by removing outliers and potentially transforming the data for a better distribution. . Outliers . I will detect outliers by filtering values according to their percentile within the dataset. A data point outside of a given percentile range will be interpreted as an outlier value and will be removed. . def PercentileOutliers(x, llim, ulim): x = np.asarray(x) if x.ndim == 2: # converting numeric parameter to list of [numeric parameter] if isinstance(llim, (int, float)): llim = [llim] * x.shape[1] if isinstance(ulim, (int, float)): ulim = [ulim] * x.shape[1] # llim/ulim is a list of percentiles with length of input columns lower = np.array( [np.percentile(x[:, i], llim[i]) for i in range(x.shape[1])]) upper = np.array( [np.percentile(x[:, i], ulim[i]) for i in range(x.shape[1])]) else: if not isinstance(llim, (int, float)): raise ValueError(f&quot;Provide a numeric `llim` for 1-d data not {type(llim)}&quot;) if not isinstance(ulim, (int, float)): raise ValueError(f&quot;Provide a numeric `ulim` for 1-d data not {type(ulim)}&quot;) lower = np.percentile(x, llim) upper = np.percentile(x, ulim) return (lower &lt; x) &amp; (x &lt; upper) # returning mask with shape of array `x` . l = len(df) df = df[PercentileOutliers( df[skew], # [price, area, rooms] llim = [1, 5, 0], ulim = [99, 95, 98] ).all(axis=1) # A row must contain no outliers to be included in the dataset ] print(f&quot;{len(df):,} rows, {l-len(df):,} rows removed&quot;) . 4,903,637 rows, 653,647 rows removed . PRICE_ADJ 2.009345 TOTAL_FLOOR_AREA 0.909617 NUMBER_HABITABLE_ROOMS 0.390720 dtype: float64 . Conclusion . I will now save the new dataset; the non-interpretable string categories have now been encoded into numeric values, and much of the outliers and instability has been removed, therefore, the data has been successfully formatted in a way in which a model can understand it to produce predictions. . json.dump({k.lower(): mappings[k] for k in df.select_dtypes(&quot;category&quot;).columns}, open(os.path.join(SERVER_DIR, &quot;encoding.json&quot;), &quot;w&quot;), indent=2) # Saving mappings for remaining categorical columns df = df.dropna() df.to_parquet(DATASET_PATH) .",
            "url": "https://uzair223.github.io/posts/house-prices/2022/05/08/p3-data-exploration.html",
            "relUrl": "/house-prices/2022/05/08/p3-data-exploration.html",
            "date": " â¢ May 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "ML Instant House Valuation: Part 2 - Data Sourcing",
            "content": "import os, shutil, zipfile from requests import Session from tqdm import tqdm import numpy as np import pandas as pd import dask.dataframe as dd from dask.diagnostics import ProgressBar . . Price paid data . HM Land Registry price paid data tracks the property sales in England and Wales submitted to HM Land Registry for registration, based on the raw data released every month, with data from 1995 to now (January 2022). . Column headers: . Price | Date of Transfer | Postcode | Property Type: D (Detached), S (Semi-Detached), T (Terraced), F (Flats/Maisonettes), O (Other) | Old/New: Y (New), N (Old) | PAON - Primary Addressable Object Name (e.g house number) | SAON - Secondary Addressable Object Name (e.g unit number) | Street | Locality | Town/City | District | . Sampling data . Initially, I will take a small chunk of the first 50 thousand rows to test and apply transformations before ultimately applying these transformations to the large dataset. Currently, I will largely be adjusting the dataset to minimise its memory usage by dropping columns and converting data types. . ppdUrl = &quot;http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv&quot; ppdNames = [ &quot;UID&quot;, &quot;PRICE&quot;, &quot;DATE_OF_TRANSFER&quot;, &quot;POSTCODE&quot;, &quot;PROPERTY_TYPE&quot;, &quot;OLD_NEW&quot;, &quot;DURATION&quot;, &quot;PAON&quot;, &quot;SAON&quot;, &quot;STREET&quot;, &quot;LOCALITY&quot;, &quot;TOWN_CITY&quot;, &quot;DISTRICT&quot;, &quot;COUNTY&quot;, &quot;PPD_CAT&quot;, &quot;RECORD_STATUS&quot; ] ppdSample = dd.read_csv(ppdUrl, names=ppdNames, dtype=object).head(n=50_000) . 50.77MB . UID PRICE DATE_OF_TRANSFER POSTCODE PROPERTY_TYPE OLD_NEW DURATION PAON SAON STREET LOCALITY TOWN_CITY DISTRICT COUNTY PPD_CAT RECORD_STATUS . 0 {5BBE9CB3-6332-4EB0-9CD3-8737CEA4A65A} | 42000 | 1995-12-21 00:00 | NE4 9DN | S | N | F | 8 | NaN | MATFEN PLACE | FENHAM | NEWCASTLE UPON TYNE | NEWCASTLE UPON TYNE | TYNE AND WEAR | A | A | . 1 {20E2441A-0F16-49AB-97D4-8737E62A5D93} | 95000 | 1995-03-03 00:00 | RM16 4UR | S | N | F | 30 | NaN | HEATH ROAD | GRAYS | GRAYS | THURROCK | THURROCK | A | A | . 2 {D893EE64-4464-44B5-B01B-8E62403ED83C} | 74950 | 1995-10-03 00:00 | CW10 9ES | D | Y | F | 15 | NaN | SHROPSHIRE CLOSE | MIDDLEWICH | MIDDLEWICH | CONGLETON | CHESHIRE | A | A | . 3 {F9F753A8-E56A-4ECC-9927-8E626A471A92} | 43500 | 1995-11-14 00:00 | TS23 3LA | S | N | F | 19 | NaN | SLEDMERE CLOSE | BILLINGHAM | BILLINGHAM | STOCKTON-ON-TEES | STOCKTON-ON-TEES | A | A | . 4 {E166398A-A19E-470E-BB5A-83B4C254CF6D} | 63000 | 1995-09-08 00:00 | CA25 5QH | S | N | F | 8 | NaN | CROSSINGS CLOSE | CLEATOR MOOR | CLEATOR MOOR | COPELAND | CUMBRIA | A | A | . Dropping empty entries and unnecessary columns . The only important columns in the dataset are the price and dates, the location data is not particularly helpful and I will be substituting them for coordinates instead. . ppdFilter = [ &quot;PRICE&quot;, &quot;DATE_OF_TRANSFER&quot;, &quot;POSTCODE&quot;, &quot;OLD_NEW&quot;, &quot;PAON&quot;, #* &quot;SAON&quot;, #* &quot;STREET&quot; #* ] #* WILL LATER BE DROPPED ppdSample = ppdSample[ppdFilter] . print(ppdSample.isna().sum()[lambda x: x &gt; 0]) l = ppdSample.shape[0] ppdSample = (ppdSample .dropna(subset=list(filter(lambda x: x != &quot;SAON&quot;, ppdFilter))) .reset_index(drop=True)) . POSTCODE 36 PAON 5 SAON 46280 STREET 750 dtype: int64 . Converting data types . ppdSample[&quot;PRICE&quot;] = ppdSample.PRICE.astype( &quot;float&quot;) # min max ~2bn | float32 - 4 bytes ppdSample[&quot;DATE_OF_TRANSFER&quot;] = dd.to_datetime( ppdSample.DATE_OF_TRANSFER) # Converting from string to datetime ppdSample[&quot;OLD_NEW&quot;] = ppdSample.OLD_NEW.astype(&quot;category&quot;) ppdDtype = ppdSample.dtypes.to_dict() ppdDtype[&quot;DATE_OF_TRANSFER&quot;] = np.dtype(&quot;O&quot;) . Address column . Merging PAON, SAON, STREET and POSTCODE into one ADDRESS column. I will apply the same formatting of &quot;SAON PAON STREET POSTCODE&quot; to the addresses in the EPC dataset in order to cross-reference and merge the datasets into one. . def ppdFormatAddr(x): return [ &#39; &#39;.join(z.strip() for z in y if isinstance(z, str)) for y in x[[&quot;SAON&quot;, &quot;PAON&quot;, &quot;STREET&quot;, &quot;POSTCODE&quot;]].values ] # List comprehension for performance . ppdSample[&quot;ADDRESS&quot;] = ppdFormatAddr(ppdSample) # Keeping POSTCODE column for geolocation purposes ppdSample = ppdSample.drop(columns=[&quot;PAON&quot;, &quot;SAON&quot;, &quot;STREET&quot;]) ppdSample.ADDRESS[:5] . 0 8 MATFEN PLACE NE4 9DN 1 30 HEATH ROAD RM16 4UR 2 15 SHROPSHIRE CLOSE CW10 9ES 3 19 SLEDMERE CLOSE TS23 3LA 4 8 CROSSINGS CLOSE CA25 5QH Name: ADDRESS, dtype: object . Processed sample set: . 50.77MB =&gt; 8.09MB . dtype memory_usage null unique . PRICE float64 | 393736 | 0.0 | 2900.0 | . DATE_OF_TRANSFER datetime64[ns] | 393736 | 0.0 | 356.0 | . POSTCODE object | 3171295 | 0.0 | 45548.0 | . OLD_NEW category | 49441 | 0.0 | 2.0 | . ADDRESS object | 4085965 | 0.0 | 49158.0 | . Index NaN | 128 | NaN | NaN | . Entire dataset . I will process and export the dataset using Dask, which utilises parallelisation accross multiple threads to enhance performance. . Pipeline . Import the entire dataset with preset datatypes and already filtered columns. | Drop rows with empty entries (ignoring SAON column which is largely empty) | Format addresses in each partition with ppdFormatAddr function and set as index | Drop duplicate addresses, keeping last. | Drop remaining unneccessary columns. | Store dataframe | ppdUrl = &quot;http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv&quot; ppdNames = [ &quot;UID&quot;, &quot;PRICE&quot;, &quot;DATE_OF_TRANSFER&quot;, &quot;POSTCODE&quot;, &quot;PROPERTY_TYPE&quot;, &quot;OLD_NEW&quot;, &quot;DURATION&quot;, &quot;PAON&quot;, &quot;SAON&quot;, &quot;STREET&quot;, &quot;LOCALITY&quot;, &quot;TOWN_CITY&quot;, &quot;DISTRICT&quot;, &quot;COUNTY&quot;, &quot;PPD_CAT&quot;, &quot;RECORD_STATUS&quot; ] ppdFilter = [ &quot;PRICE&quot;, &quot;DATE_OF_TRANSFER&quot;, &quot;POSTCODE&quot;, &quot;OLD_NEW&quot;, &quot;PAON&quot;, &quot;SAON&quot;, &quot;STREET&quot; ] ppdDtype = { &quot;PRICE&quot;: np.float64, &quot;DATE_OF_TRANSFER&quot;: object, &quot;POSTCODE&quot;: object, &quot;OLD_NEW&quot;: pd.CategoricalDtype(), &quot;PAON&quot;: object, &quot;SAON&quot;: object, &quot;STREET&quot;: object } def ppdFormatAddr(x): return [ &#39; &#39;.join(z.strip() for z in y if isinstance(z, str)) for y in x[[&quot;SAON&quot;, &quot;PAON&quot;, &quot;STREET&quot;, &quot;POSTCODE&quot;]].values ] # List comprehension for performance . ppdPipeline = (dd .read_csv(ppdUrl, names=ppdNames, usecols=ppdFilter, parse_dates=[&quot;DATE_OF_TRANSFER&quot;], dtype=ppdDtype) .dropna(subset=filter(lambda x: x != &quot;SAON&quot;, ppdFilter)) .map_partitions(lambda x: x.assign(ADDRESS=ppdFormatAddr)) .drop_duplicates(subset=&quot;ADDRESS&quot;,keep=&quot;last&quot;) .map_partitions(lambda x: x.set_index(&quot;ADDRESS&quot;)) .drop(columns=[&quot;SAON&quot;, &quot;PAON&quot;, &quot;STREET&quot;]) ) . if not os.path.exists(PPD_PATH) and not os.path.exists(DATASET_PATH): with ProgressBar(): ppdPipeline.to_parquet(PPD_PATH, compression=&quot;snappy&quot;) ppd = dd.read_parquet(PPD_PATH) . 2.46GB 14,876,050 rows . PRICE DATE_OF_TRANSFER POSTCODE OLD_NEW . ADDRESS . 87 THEOBALD ROAD NR1 2NX 37500.0 | 1995-06-30 | NR1 2NX | N | . 16 AFON RHOS ESTATE LL55 4SE 44500.0 | 1995-11-22 | LL55 4SE | N | . 69 CRANHILL ROAD BA16 0BZ 48500.0 | 1995-04-13 | BA16 0BZ | N | . 4 BRIARY LANE SG8 9BZ 53000.0 | 1995-01-06 | SG8 9BZ | N | . 133 WOOD LANE CH5 3JF 67500.0 | 1995-12-12 | CH5 3JF | N | . Energy Performance of Buildings Data . Energy Performance of Buildings Data provides access to Energy Performance Certificates and Display Energy Certificate data for buildings across England and Wales, based on data released quarterly from 1 October 2008 up to 30 September 2021. The data has to be downloaded manually as it requires authentication in order to access the site. . I will only be requiring the EPC data, which has 90 column headers including: . Address headers (Address lines 1 to 3) | Postcode | Number of habitable rooms | Total square footage | Property type | . Sampling data . I will be following a very similar workflow as to processing the Price Paid data. Initially, I will use the first certificate file as a sample set and filter out most unnecessary columns. I will then convert the data types of the remaining columns. . from glob import glob epcPath = os.path.join(RAW_PATH, &quot;all-domestic-certificates&quot;, &quot;*&quot;, &quot;certificates.csv&quot;) epcSample = pd.read_csv(glob(epcPath)[0], low_memory=False) . 207.64MB . LMK_KEY ADDRESS1 ADDRESS2 ADDRESS3 POSTCODE BUILDING_REFERENCE_NUMBER CURRENT_ENERGY_RATING POTENTIAL_ENERGY_RATING CURRENT_ENERGY_EFFICIENCY POTENTIAL_ENERGY_EFFICIENCY ... LOCAL_AUTHORITY_LABEL CONSTITUENCY_LABEL POSTTOWN CONSTRUCTION_AGE_BAND LODGEMENT_DATETIME TENURE FIXED_LIGHTING_OUTLETS_COUNT LOW_ENERGY_FIXED_LIGHT_COUNT UPRN UPRN_SOURCE . 0 208945009062019011009444285518421 | 15, Elmfield Road | NaN | NaN | GL51 9JJ | 2864085568 | C | B | 71 | 84 | ... | Cheltenham | Cheltenham | CHELTENHAM | England and Wales: 1950-1966 | 2019-01-10 09:44:42 | rental (private) | NaN | NaN | 1.001204e+11 | Address Matched | . 1 1296022739962015031912051214438915 | 15, Lime Close | Prestbury | NaN | GL52 3EF | 1185324378 | E | C | 53 | 79 | ... | Cheltenham | Tewkesbury | CHELTENHAM | England and Wales: 1967-1975 | 2015-03-19 12:05:12 | owner-occupied | NaN | NaN | 1.001204e+11 | Address Matched | . 2 1102892409262017040613095870338213 | 44, Naunton Lane | NaN | NaN | GL53 7BH | 1795060278 | D | B | 65 | 85 | ... | Cheltenham | Cheltenham | CHELTENHAM | England and Wales: 1900-1929 | 2017-04-06 13:09:58 | owner-occupied | NaN | NaN | 1.001204e+11 | Address Matched | . 3 2383300702009033116544446217998 | Flat 8, Hazelhurst | 24, Eldorado Road | NaN | GL50 2PT | 9774302468 | E | D | 46 | 57 | ... | Cheltenham | Cheltenham | CHELTENHAM | England and Wales: 1900-1929 | 2009-03-31 16:54:44 | rental (private) | NaN | NaN | 1.000048e+10 | Address Matched | . 4 1656454289042018082020372657982008 | 6, Charles Street | NaN | NaN | GL51 9HH | 4037189578 | C | B | 71 | 87 | ... | Cheltenham | Cheltenham | CHELTENHAM | England and Wales: before 1900 | 2018-08-20 20:37:26 | owner-occupied | NaN | NaN | 1.001204e+11 | Address Matched | . 5 rows Ã 92 columns . Filtering columns and dropping empty entries . As of now there are far too many unneccessary columns, I will only take the columns I think will potentially be useful as features for predictive models. Additionally, I will be replacing placeholder values such as &quot;NO DATA!&quot; to NaN values and will subsequently drop them. . epcFilter = [ &quot;ADDRESS1&quot;, #* &quot;ADDRESS2&quot;, #* &quot;POSTCODE&quot;, #* &quot;PROPERTY_TYPE&quot;, &quot;BUILT_FORM&quot;, &quot;TOTAL_FLOOR_AREA&quot;, &quot;NUMBER_HABITABLE_ROOMS&quot;, &quot;EXTENSION_COUNT&quot;, &quot;GLAZED_TYPE&quot;, &quot;CONSTRUCTION_AGE_BAND&quot;, &quot;TENURE&quot; ] #* WILL LATER BE DROPPED epcSample = epcSample[epcFilter] . nanValues = [ &quot;NULL&quot;, &quot;INVALID&quot;, &quot;INVALID!&quot;, &quot;NODATA!&quot;, &quot;NO DATA!&quot;, &quot;N/A&quot;, &quot;Not applicable&quot;, &quot;Not recorded&quot;, &quot;not defined&quot;, &quot;Blank&quot;, ] epcSample[epcSample.isin(nanValues)] = np.nan print(epcSample.isna().sum()[lambda x: x &gt; 0]) epcSample = (epcSample .dropna(subset=list(filter(lambda x: x != &quot;ADDRESS2&quot;, epcFilter))) .reset_index(drop=True)) . ADDRESS2 25366 BUILT_FORM 1141 NUMBER_HABITABLE_ROOMS 4471 EXTENSION_COUNT 4471 GLAZED_TYPE 9938 CONSTRUCTION_AGE_BAND 4460 TENURE 1432 dtype: int64 . Converting data types . float_col = [&quot;TOTAL_FLOOR_AREA&quot;, &quot;NUMBER_HABITABLE_ROOMS&quot;, &quot;EXTENSION_COUNT&quot;] cat_col = [ &quot;PROPERTY_TYPE&quot;, &quot;BUILT_FORM&quot;, &quot;GLAZED_TYPE&quot;, &quot;CONSTRUCTION_AGE_BAND&quot;, &quot;TENURE&quot; ] epcSample[float_col] = epcSample[float_col].astype(&quot;float&quot;) epcSample[cat_col] = epcSample[cat_col].astype(&quot;category&quot;) epcDtype = epcSample.dtypes.to_dict() . Formatting addresses . &quot;ADDRESS1 ADDRESS2 POSTCODE&quot; all in uppercase, commas removed from text. . def epcFormatAddr(x): return [ &#39; &#39;.join(z.strip().replace(&quot;,&quot;, &quot;&quot;).upper() for z in y if not pd.isna(z)) for y in x[[&quot;ADDRESS1&quot;, &quot;ADDRESS2&quot;, &quot;POSTCODE&quot;]].values ] # List comprehension for performance . epcSample[&quot;ADDRESS&quot;] = epcFormatAddr(epcSample) epcSample = epcSample.drop(columns=[&quot;ADDRESS1&quot;, &quot;ADDRESS2&quot;, &quot;POSTCODE&quot;]) epcSample.ADDRESS[:5] . 0 15 ELMFIELD ROAD GL51 9JJ 1 15 LIME CLOSE PRESTBURY GL52 3EF 2 44 NAUNTON LANE GL53 7BH 3 6 CHARLES STREET GL51 9HH 4 25 BELWORTH COURT GL51 6HQ Name: ADDRESS, dtype: object . Processed sample set: . 207.64MB =&gt; 4.50MB . dtype memory_usage null unique . PROPERTY_TYPE category | 38719 | 0.0 | 5.0 | . BUILT_FORM category | 38823 | 0.0 | 6.0 | . TOTAL_FLOOR_AREA float64 | 305808 | 0.0 | 7559.0 | . NUMBER_HABITABLE_ROOMS float64 | 305808 | 0.0 | 21.0 | . EXTENSION_COUNT float64 | 305808 | 0.0 | 5.0 | . GLAZED_TYPE category | 39180 | 0.0 | 8.0 | . CONSTRUCTION_AGE_BAND category | 39895 | 0.0 | 13.0 | . TENURE category | 39179 | 0.0 | 8.0 | . ADDRESS object | 3390932 | 0.0 | 30564.0 | . Index NaN | 128 | NaN | NaN | . Entire dataset . I will be process and export the entire dataset using Dask with a similar pipeline to that of the Price-paid data. . Pipeline . Import all files as a glob using Dask. | Drop rows with empty entries (ignoring ADDRESS2 column) | Convert data types | Format addresses with epcFormatAddr function | Drop duplicate addresses and set as index | Drop unnecessary columns | epcFilter = [ &quot;ADDRESS1&quot;, &quot;ADDRESS2&quot;, &quot;POSTCODE&quot;, &quot;PROPERTY_TYPE&quot;, &quot;BUILT_FORM&quot;, &quot;TOTAL_FLOOR_AREA&quot;, &quot;NUMBER_HABITABLE_ROOMS&quot;, &quot;EXTENSION_COUNT&quot;, &quot;GLAZED_TYPE&quot;, &quot;CONSTRUCTION_AGE_BAND&quot;, &quot;TENURE&quot; ] nanValues = [ &quot;NULL&quot;, &quot;INVALID&quot;, &quot;INVALID!&quot;, &quot;NODATA!&quot;, &quot;NO DATA!&quot;, &quot;N/A&quot;, &quot;Not applicable&quot;, &quot;Not recorded&quot;, &quot;not defined&quot;, &quot;Blank&quot;, ] epcDtype = { &#39;ADDRESS1&#39;: object, &#39;ADDRESS2&#39;: object, &#39;POSTCODE&#39;: object, &#39;PROPERTY_TYPE&#39;: pd.CategoricalDtype(), &#39;BUILT_FORM&#39;: pd.CategoricalDtype(), &#39;TOTAL_FLOOR_AREA&#39;: np.float64, &#39;NUMBER_HABITABLE_ROOMS&#39;: np.float64, &#39;EXTENSION_COUNT&#39;: np.float64, &#39;GLAZED_TYPE&#39;: pd.CategoricalDtype(), &#39;CONSTRUCTION_AGE_BAND&#39;: pd.CategoricalDtype(), &#39;TENURE&#39;: pd.CategoricalDtype() } def epcFormatAddr(x): return [ &#39; &#39;.join(z.strip().replace(&quot;,&quot;, &quot;&quot;).upper() for z in y if not pd.isna(z)) for y in x[[&quot;ADDRESS1&quot;, &quot;ADDRESS2&quot;, &quot;POSTCODE&quot;]].values ] # List comprehension for performance . epcPipeline = (dd .read_csv(epcPath, usecols=epcFilter, na_values=nanValues, dtype=epcDtype) .dropna(subset=list(filter(lambda x: x != &quot;ADDRESS2&quot;, epcFilter)), how=&quot;any&quot;) .map_partitions(lambda x: x.assign(ADDRESS=epcFormatAddr)).drop_duplicates(subset=&quot;ADDRESS&quot;,keep=&quot;last&quot;) .map_partitions(lambda x: x.set_index(&quot;ADDRESS&quot;)) .drop(columns=[&quot;ADDRESS1&quot;, &quot;ADDRESS2&quot;, &quot;POSTCODE&quot;]) ) . if not os.path.exists(EPC_PATH) and not os.path.exists(DATASET_PATH): with ProgressBar(): epcPipeline.to_parquet(EPC_PATH, compression=&quot;snappy&quot;) epc = dd.read_parquet(EPC_PATH) . 1.64GB 14,148,102 rows . PROPERTY_TYPE BUILT_FORM TOTAL_FLOOR_AREA GLAZED_TYPE EXTENSION_COUNT NUMBER_HABITABLE_ROOMS CONSTRUCTION_AGE_BAND TENURE . ADDRESS . 54 CAISTOR DRIVE TS25 2QG Bungalow | Semi-Detached | 59.0 | double glazing, unknown install date | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | . 18 TRENTBROOKE AVENUE TS25 5JN Bungalow | Semi-Detached | 70.8 | double glazing, unknown install date | 0.0 | 4.0 | England and Wales: 1950-1966 | owner-occupied | . 3 HARLECH WALK TS26 0TN House | Mid-Terrace | 86.0 | double glazing installed before 2002 | 1.0 | 5.0 | England and Wales: 1950-1966 | owner-occupied | . 31 LAUREL GARDENS TS25 4NZ Flat | Detached | 52.0 | double glazing installed during or after 2002 | 0.0 | 3.0 | England and Wales: 2007 onwards | rental (social) | . 16 HEADINGLEY COURT SEATON CAREW TS25 2PD House | Detached | 121.0 | double glazing installed before 2002 | 1.0 | 6.0 | England and Wales: 1991-1995 | owner-occupied | . Merging datasets . merge = (dd .merge(ppd, epc, how=&quot;inner&quot;, left_index=True, right_index=True) # intersection of price-paid and certificate data .reset_index(drop=True) # dropping address index as it is no longer necessary .map_partitions(lambda x: x.set_index(&quot;DATE_OF_TRANSFER&quot;).sort_index()) # set date to new index ) . if not os.path.exists(MERGED_PATH): with ProgressBar(): merge.to_parquet(MERGED_PATH, schema=&quot;infer&quot;, compression=&quot;snappy&quot;) df = dd.read_parquet(MERGED_PATH).compute() # Computing into memory since it will use ~600mb . 614.61MB 5,566,962 rows . PRICE POSTCODE OLD_NEW PROPERTY_TYPE BUILT_FORM TOTAL_FLOOR_AREA GLAZED_TYPE EXTENSION_COUNT NUMBER_HABITABLE_ROOMS CONSTRUCTION_AGE_BAND TENURE . DATE_OF_TRANSFER . 1995-01-01 16000.0 | HX1 4NG | N | House | Enclosed Mid-Terrace | 99.00 | double glazing, unknown install date | 0.0 | 4.0 | England and Wales: before 1900 | owner-occupied | . 1995-01-02 35000.0 | NN1 4LL | N | House | Mid-Terrace | 85.00 | double glazing installed before 2002 | 1.0 | 4.0 | England and Wales: 1900-1929 | owner-occupied | . 1995-01-03 15000.0 | NE34 8TF | N | House | Semi-Detached | 74.00 | double glazing installed before 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | . 1995-01-03 48000.0 | HR8 2DB | N | Bungalow | Detached | 78.00 | double glazing installed before 2002 | 1.0 | 3.0 | England and Wales: 1950-1966 | owner-occupied | . 1995-01-03 82000.0 | TR13 8BP | N | Bungalow | Detached | 118.88 | double glazing installed during or after 2002 | 0.0 | 4.0 | England and Wales: 1967-1975 | rental (private) | . Downloading additional data for feature engineering . During my dataset analysis, I will require additional data to further transform my dataset. This includes: . Geographical data from the National Statistics Postcode Lookup (February 2022), e.g longitude, latitude | HPI (House Price Index) data from National Statistics UK House Price Index (January 2022) | . from tqdm import tqdm class Downloader: def __init__( self, _session, pbar_enabled: bool = False, pbar_kwargs={}, ): self.session = _session self.pbar_enabled = pbar_enabled self.pbar_kwargs = pbar_kwargs def dlFile(self, file, chunk_size=1024, **reqKwargs) -&gt; None: r = self.session.get( **reqKwargs, stream=True) # Sending get request to provided url r.raise_for_status() # raise error if request failure if self.pbar_enabled: pbar = tqdm(desc=file, total=int(r.headers.get(&quot;content-length&quot;)), unit=&quot;iB&quot;, unit_scale=True, **self.pbar_kwargs) # Initialising progress bar with open(file, &quot;wb&quot;) as f: # Iterating chunks as they come from server for chunk in r.iter_content(chunk_size=chunk_size): f.write(chunk) # Writing chunk to file f.flush() # Flushing input buffer if self.pbar_enabled: pbar.update(len(chunk)) # Updating progress bar by size of chunk if self.pbar_enabled: pbar.close() . . Postcode Lookup . if not os.path.exists(PCD_PATH): url = &quot;https://www.arcgis.com/sharing/rest/content/items/1a0444ee3c43452ea16c530966ae8984/data&quot; extractPath = PCD_PATH.removesuffix(&quot;.csv&quot;) if not os.path.exists(extractPath): zipPath = extractPath + &quot;.zip&quot; # downloading zip file from website with Session() as sess: Downloader(sess, pbar_enabled=True).dlFile(zipPath, url=url) # extracting zip to folder with zipfile.ZipFile(zipPath, &quot;r&quot;) as handle: handle.extractall(extractPath) os.remove(zipPath) # removing zip file # taking necessary columns and writing into csv file (pd .read_csv(os.path.join(extractPath, &quot;Data&quot;, &quot;NSPL_FEB_2022_UK.csv&quot;), usecols=[&quot;pcds&quot;, &quot;lat&quot;, &quot;long&quot;]) .rename(columns={&quot;long&quot;:&quot;lng&quot;}) .rename(columns=lambda x: x.upper()) .to_csv(PCD_PATH, index=False) ) shutil.rmtree(extractPath) # removing folder geo = pd.read_csv(PCD_PATH) . 214.96MB 2,673,018 rows . PCDS LAT LNG . 0 AB1 0AA | 57.101474 | -2.242851 | . 1 AB1 0AB | 57.102554 | -2.246308 | . 2 AB1 0AD | 57.100556 | -2.248342 | . 3 AB1 0AE | 57.084444 | -2.255708 | . 4 AB1 0AF | 57.096656 | -2.258102 | . House Price Index . if not os.path.exists(HPI_PATH): url = &quot;http://publicdata.landregistry.gov.uk/market-trend-data/house-price-index-data/Indices-2022-02.csv&quot; # Parsing file and writing to csv file (pd.read_csv(url, usecols=[&quot;Date&quot;, &quot;Region_Name&quot;, &quot;Index&quot;],parse_dates=[&quot;Date&quot;]) .query(&quot;Region_Name == &#39;England and Wales&#39;&quot;, engine=&quot;python&quot;) .set_index(&quot;Date&quot;).sort_index() .Index.to_csv(HPI_PATH) ) hpi = pd.read_csv(HPI_PATH) . 24.58KB 326 rows . Date Index . 0 1995-01-01 | 26.440075 | . 1 1995-02-01 | 26.369132 | . 2 1995-03-01 | 26.401129 | . 3 1995-04-01 | 26.604794 | . 4 1995-05-01 | 26.645437 | . Conclusion . All the necessary data has now been collected locally, however it is still unusable and thus useless; I will now move onto exploring and analysing the data, performing transformations along the way, in order to make the data better fit and explain the price data. .",
            "url": "https://uzair223.github.io/posts/house-prices/2022/05/08/p2-data-sourcing.html",
            "relUrl": "/house-prices/2022/05/08/p2-data-sourcing.html",
            "date": " â¢ May 8, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "ML Instant House Valuation: Part 1 - Analysis",
            "content": "Background . I will be creating a house valuation web app which will allow a user to input a set of property features (such as the number of rooms, floor size, property age etc.) and be given an instant valuation based on a machine learning solution. The idea began by brainstorming a number of machine learning related tools; first a weather forecasting tool, then a stock price predictor, but finally I settled with a house price valuation tool. I found it more interesting and challenging to be forced to build, analyse and develop a solution myself, as I could not find material/tutorials for this project online, unlike the many tutorials available for weather nad stock. The project began more as a research task but I decided to develop it as a web service, after stumbling across this Diamond price prediction project (now inactive) online. . Current systems analysis . To get an idea of how my system should function, I will take a look at popular pre-existing house valuation tools. Ideally, I would like an indication to the data and algorithms behind the various tools, however it is unlikely that I will be able to. While the UI and aesthetics is not a main focus of the analysis, I will look at and compare the design components used across the different tools, and possibly incorporate and expand them into my final design. . Zoopla . . I like Zoopla&#39;s system as it is very simple to use and understand, its UI is visually appealing, and it includes other data apart from just the valuation, such as local amneties, making it even more helpful to an end user. The Zoopla valuation uses data already on its databases to instantly valuate the property. Its additional data sources, such as the HM Land Registry, are also provided, which is ideal for my problem as it will allow me to explore and potentially use datasets from the same reliable sources. Unlike Zoopla, I will adopt a system where the end user is asked for the propertyâs location as well as other property features, such as its total square footage etc., which allow for the valuation of properties that possibly donât even exist. Additionally, I will provide the last sold price if the input property does exist, similar to how Zoopla does. . Yopa . . I like the simplicity of the Yopa valuation, however only requiring the number of bedrooms and location doesn&#39;t seem like it would yield a very accurate result. In my system, I would ideally require at least 5 features for the resulting estimation. Yopa, similar to Zoopla, also implements a low and high estimation alongside its valuation, and I will also attempt to implement this. . OnTheMarket . . I particularly like the UI and aesthetic which OnTheMarket&#39;s online valuation system uses. The predicted prices seem to be based on location averages, which I may also implement in my system alongside the machine learning aspect. Additionally, the system requires 15 features in addition to the location, which makes the valuation seem more accurate to the end user, unlike the Yopa valuation which only required the number of bedrooms. I will however reduce the number of required features, and keep all my parameters on a single page, to make the process somewhat easier. I will source data which includes similar features such as the property age, number of rooms, property type etc. . End User Questionnaire . I asked 3 home-owners questions on their thoughts of the project and how I should develop it; I think the responses of potential users will be particularly helpful in the direction of which the development of my project goes: . Would you consider an online property valuation as an accurate way to get the price of a property? . Response 1: I would consider an online valuation to get a rough idea of a house&#39;s value. I would probably use a tool like this before contacting an estate agent to valuate the property in real life. | Response 2: I&#39;d use an instant valuation tool to see if a property is worth pursuing, it would save a lot of time instead of having to contact somebody myself. | Response 3: I don&#39;t think a service like this would be highly accurate but I do think it would give a useful estimation and could give a relatively reliable prediction. . | . | What characteristics of a property do you think would most impact its valuation? . Response 1: I think the age, size and property type would be useful in telling the price of a house. However, I believe the value of properties in the area would have greatest impact on a house&#39;s price. | Response 2: The price of surrounding properties would be the most useful in the valuation of a house, along with its total square footage, number of rooms, and perhaps general local area information like population and crime rate. | Response 3: The quality of the neighbourhood of a property and the prices of other houses nearby are the most indicative of a property&#39;s value. Property features such as the size and the type also impact a property&#39;s valuation greatly. . | . | What additional features would you like to see, alongside the valuation? . Response 1: I&#39;d like to see the average price of houses in the local area, or perhaps the average price of similar property types. | Response 2: I think it&#39;d be helpful to see local area statistics such as the population and average house price in the area. Additionally, I don&#39;t think a point estimation is a very helpful so I&#39;d like to see an valuation with an associated interval, with a low and high estimation. | Response 3: It would be interesting to get predictions for the price of a property years in the future. It would also be helpful to see the previous sold price of a property. . | . | How would you like the information displayed to you? . Response 1: I&#39;d like to see the information displayed as simply as it can, clutter and unnecessary information can be quite annoying - like asking for email and other data that has nothing to do with the actual predictions. I would prefer all the information easily visible in one window. | Response 2: I think displaying a map of the property&#39;s area along with its valuation would be helpful, especially for people who consider moving into the area. There should be an option to have the predictions emailed to you, but not as a requirement. | Response 3: A simple web form should suffice for inputting the property details. The output could be given in a separate page or pop-up with all the information. . | . | In conclusion, the home-owners would consider using the service as a rough estimate for a property&#39;s value before seeking professional advice. All 3 participants think that local area information could be useful in the valuation, as well as the property size and type. I liked the idea to include local area statistics in the output to the user, and I agree that unnecessary clutter should be avoided when serving the information to the end user. . Problem Modelling . My initial thoughts on how to approach the problem. I will try implement the system in four (or potentially five) stages: . Stage 1. Data sourcing - collecting and stitching data from various sources to build a large dataset (aiming for around 1 million rows). | Stage 2. Data sanitisation and preprocessing - transforming the dataset and its features to allow for predictive modelling | Stage 3. Model training - testing and training various machine learning models on the dataset to maximise accuracy | Stage 4. Web development - implementing the server and client side for users to access. | (if possible) Stage 5. App deployment - deploying the client-server model to the web for potential use. | . Below covers the general implementation and structure of the project, split into its distinct stages. . . I am currently unclear on what machine learning models to use and how to preprocess the data. These decisions will be made as the data is collected and in exploratory data analysis. I will fit multiple models on the dataset and test their performance to pick an algorithm, I may also ensemble/stack multiple models if it proves useful. I may need a database for features of the model which require context from the inputs provided by the end user, such as geospatial or local area data. . The web service itself, which will serve the predictions to the end user, will look somewhere along the lines of: . . Drawing inspiration from the Zoopla and OnTheMarket user interfaces, with a simple form, an interval prediction, and a map of the area using Google Maps API. . Algorithms . Conformal prediction . Conformal prediction is a technique used to assess the uncertainty of predictions produced by a machine learning model. In particular, given an input, conformal prediction estimates a prediction interval in regression problems and a set of classes in classification problems. Both the prediction interval and sets cover the true value with high probability (usually provided by the user). Steps to producing a conformal prediction model: . Identify a score function to measure the discrepancy between model outputs $ hat y$ and expected outputs $y$. This metric is critical as it decides what prediction sets can be obtained. For instance, in regression problems, we could take the absolute error $| hat y - y|$ as the score function. | Compute nonconformity scores for the model by splitting the training set into a proper set and calibration set. The model is only trained on the proper training set; and scores are computed on the calibration set where $S_n = S( hat y_n, y_n)$ for the $n$th sample. | Prediction - use the underlying algorithm to make a point prediction $ hat y$ and take the ($1 - alpha$)th quantile $Q$ from the score set - where $ alpha$ is a user-provided significance level. Output $ hat y pm Q$ as the model&#39;s prediction. | Stochastic gradient descent and linear regression . Regression models are used to describe relationships between variables by fitting a line to the observed data. Multiple linear regression is used to estimate the relationship between multiple independent variables and one dependent variable. The model makes assumptions about the dataset: . Homogeneity of variance (homoscedasticity) - the observations in the dataset must be about the same distance from the regression line. . . | Independence of observations - there are no hidden relationships among independent variables. . | Normality - the data follows a normal distribution. | Linearity - the line of best fit through the data points is a straight line, not some polynomial/exponential curve | . Multiple linear regression generalises a dataset as the output being a weighted summation of its inputs. $$ hat y_i = b + sum^n_{k=0} w_k x_{ik} $$ In matrix form, it can be written as a dot product, as shown: $$ hat y = w cdot x + b $$ Where, $ hat y$: predicted $x$: independent variables vector $w$: coefficients vector i.e weights $b$: constant term i.e bias . The stochastic gradient descent algorithm is used to adjust the weights and bias for the model. The algorithm requires a differentiable cost function in order to derive the gradients of each adjustable parameter. I will be using the half mean squared cost function, as it will differentiate nicely making it a lot easier to implement. The cost function is defined as such, $$ J(w,b) = frac 1{2n} sum_{i=1}^n(wx_i + b - y_i)^2 $$ The goal is to find the values of the $w$ and $b$ terms which minimise the error. This can be done by taking the partial derivative (gradient) of the cost function $J$ with respect each parameter, $$ frac { partial J}{ partial b} = frac 1n sum_{i=1}^n ( hat{y}_i - y_i) frac { partial J}{ partial w} = frac 1n sum_{i=1}^n x_i( hat{y}_i - y_i) = frac 1n x cdot ( hat{y} - y) $$ Where $ eta$ is the learning rate, iteratively update the parameters until they converge, $$ b := b - eta cdot partial b w := w - eta cdot partial w $$ . The implementation of linear regression with stochastic gradient descent will be a great starting point when implementing the more complex multilayer perceptron model, which similarly uses the weighted summation function to make predictions and a gradient descent algorithm to optimise its parameters. . Multilayer Perceptron . A multilayer perceptron is a feed-forward neural network consisting of at least three layers of nodes; an input layer, a hidden layer, and an output layer. . . The input layer receives an input signal to be processed. The model&#39;s prediction is performed by the output layer. The hidden layers between the input and output layer are the computational engine of the model. MLPs can be used to approximate any continuous function, and can solve problems which are not linearly separable by using non-linear activation functions. . . An MLP learns in two stages: . Forward propagation, where data is fed from the input layer through the hidden layers to the output layer. For a general model of $L$ layers with weights $W$ and activation functions $ sigma$, forward propagation can be expressed as a function composition as shown: $$ F(x) = sigma_N(W_L cdot sigma_{L-1}(W_{L-1} cdots sigma_2(W_2 cdot sigma_1(W_1 cdot x)) cdots)) $$ | Backpropagation, where by using the chain rule, the partial derivatives of the loss function w.r.t. the various weights and biases are fed back through the network from the output layer through to the input layer. The differentiation produces a gradient along which the parameters may be adjusted towards the minima. See Backpropagation algorithm for more detail. . . | . These stages are repeated until the loss converges towards a minimum. . Backpropagation algorithm . Backpropagation is a widely used algorithm for training feed-forward neural networks used to compute the gradient of a loss function with respect to the weights of a network for a training set of input-output examples. The gradient computed by backpropagation can be used with other algorithms such as Stochastic Gradient Descent (or its derivatives) to update the weights and minimise error in a network. . Below shows the mathematical logic behind the algorithm. The following example is for a feed-forward network of $L$ layers. . $$ F(x) = sigma_N(W_L cdot sigma_{L-1}(W_{L-1} cdots sigma_2(W_2 cdot sigma_1(W_1 cdot x)) cdots)) Cost = J(y, F(x)) $$For the model above, the derivative of the loss w.r.t. the input $x$ using the chain rule is generalised as such: $$ text{Chain rule}: F(x) = f(g(x)) qquad F&#39;(x) = f&#39;(g(x)) cdot g&#39;(x) text{derivative of compound function} = text{derivative of outer function} times text{derivative of inner function} [1em] therefore quad frac{dJ}{dx} = frac{dJ}{da_L} circ sigma_L&#39; cdot W_L circ sigma_{L-1}&#39; cdot W_{L-1} cdots circ sigma_1&#39; cdot W_1 $$ The gradient $ nabla$ is the transpose of the derivative of the loss w.r.t. the input, therefore, the matrices are transposed and the order of multiplication is reversed, such that: $$ nabla_xJ = W_1^T cdot sigma_1&#39; cdots circ W_{L-1}^T cdot sigma_{L-1}&#39; circ W_L^T cdot sigma_L&#39; circ nabla_{a_L}J $$ The backpropagation consists of then evaluating this expression from right to left, i.e starting from the output layer, computing the gradient at each layer on the way. To avoid unneccessary repeated computation of expressions, we can introduce a new variable $ delta_l$, the &quot;error at layer $l$&quot;. This variable is a vector with length of the number of nodes in its associated layer. Each value can be interpreted as the loss attributable to that node for the given input-output sample. $ delta_l$ can then be computed recursively, starting backwards from layer $L rightarrow 1$, such that: $$ delta_{l-1} := sigma_{l-1}&#39; circ W_l^T cdot delta_l$$ The gradients of the weights can then be calculated, such that: $$ nabla_{W_l}J = delta_l cdot a_{l-1}^T$$ Finally, the weights be updated (demonstrated here with stochastic gradient descent), such that: $$ W_l := W_l - eta cdot nabla_{W_l}J $$ . Where, $L$ : Number of layers in network $ sigma$ : Non-linear activation function $J$ : Loss function taking parameters $y$ (expected output), and $ hat y$ (actual output) $W_l$ : Weight matrix of $l$th layer; all the connections between nodes in current and next layer. $a_l$ : Activated output of $l$th layer $ delta_l$ : Error at $l$th layer $ eta$ : Learning rate of SGD algorithm $A^T$ : Transpose of matrix $A$ $ nabla_ba$ : Gradient of $a$ w.r.t $b$ $ circ$ : Hadamard (element-wise) matrix multiplication, products taken from left to right . CART algorithm . The CART (Classification and Regression Tree) algorithm is a divide-and-conquer algorithm used to build a decision tree (as a predictive model) to predict the output of a given sample, using attributes observed in a given training set. The model is very intuitive and easy to interpret/explain. Furthermore, it is robust and does not require any prior normalisation or scaling of the dataset. It is also unaffected by outliers and finds important variables automatically, unlike linear regression models. However, decision trees are prone to overfitting, and small changes to the dataset can cause a large change in tree structure causing instability. The algorithm, while can be modelled to perform the task, is inadequate for applying regression and predicting continuous values, and cannot extrapolate beyond its observed set. To mitigate some of these issues, decision trees are often used in ensemble methods including random forests and gradient boosting machines, which aim to reduce overfitting. . The algorithm performs binary splits using the features of a dataset to build a tree, until all of the nodes are ideally &quot;pure&quot;. . . Optimal splits are found by comparing the impurity of multiple potential splits. Popular metrics used to measure impurity are: . Gini impurity (classification) - calculates the probability that a specific feature is classified incorrectly when selected randomly. A pure node of gini index 0 indicates that all its contained elements are of one unique class. $$ textit{Gini} = 1 - sum^n_{k=1}P_k^2 $$ Where $P_i$ is the probability of class $k$. . | Variance reduction (regression) - variance is used for calculating the homogeneity of a node. If a node is entirely homogeneous, i.e pure, then the variance is zero. $$ textit{Var}(y) = frac 1n sum^n_{i=1}(y_i- bar y)^2 $$ Where $ bar y$ is the mean of input data $y$ . | . . To determine the quality of a split using some impurity metric, we use information gain, which can be described as the reduction of information entropy $H$ from a prior state (parent node) to a new state (split into children nodes): $$ text{Gain} = text{Entropy of parent node} - text{Average entropy of children nodes} IG(p,l,r) = H(p) - frac 1{n_p} (n_lH(l) + n_rH(r)) $$ Where, $p$: parent node $l$: left subtree $r$: right subtree $n_x$: number of elements in a set $x$ $H(x)$: entropy metric function . The algorithm starts at the root node, with the entire dataset. It then compares potential splits (all permutations of features and unique thresholds) and chooses the split with the highest information gain. It then recursively repeats this process on the its resulting splits until the nodes are pure, or until the process is stopped manually. The algorithm is stopped by user specified parameters, such as when the tree grows to a maximum depth provided or if a split produces a minimum number of samples. . Random forest . Random forests are a bagging ensemble learning method which construct multiple decision trees at training time. In the bagging method, also known as bootstrap aggregation, a random sample from the training set is selected with replacement - allowing data points to be used multiple times. After several data samples are generated, a number of estimators are then individually trained. When making a prediction, outputs from each indiviual tree are aggregated to yield a more accurate result; for regression the mean is taken, and for classification the majority vote. . . Gradient boosting machine . Gradient boosting is a boosting ensemble learning method that combines a set of weak learners (decision trees) into a single strong learner to minimise training errors. In boosting, a random sample of data is selected, fitted with a model and trained sequentially; each model tries to improve the errors of its predecessor, by training on the previous predictor&#39;s residual errors. The method combines both the gradient descent and boosting methods, hence the name gradient boosting. . .",
            "url": "https://uzair223.github.io/posts/house-prices/2022/05/08/p1-analysis.html",
            "relUrl": "/house-prices/2022/05/08/p1-analysis.html",
            "date": " â¢ May 8, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://uzair223.github.io/posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ âsitemap.xmlâ | absolute_url }} | .",
          "url": "https://uzair223.github.io/posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}